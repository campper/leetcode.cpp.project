[
  {"id":"ChineseTextRecognitionYu2309.01083","abstract":"Scene text recognition has been studied for decades due to its broad applications. However, despite Chinese characters possessing different characteristics from Latin characters, such as complex inner structures and large categories, few methods have been proposed for Chinese Text Recognition (CTR). Particularly, the characteristic of large categories poses challenges in dealing with zero-shot and few-shot Chinese characters. In this paper, inspired by the way humans recognize Chinese texts, we propose a two-stage framework for CTR. Firstly, we pre-train a CLIP-like model through aligning printed character images and Ideographic Description Sequences (IDS). This pre-training stage simulates humans recognizing Chinese characters and obtains the canonical representation of each character. Subsequently, the learned representations are employed to supervise the CTR model, such that traditional single-character recognition can be improved to text-line recognition through image-IDS matching. To evaluate the effectiveness of the proposed method, we conduct extensive experiments on both Chinese character recognition (CCR) and CTR. The experimental results demonstrate that the proposed method performs best in CCR and outperforms previous methods in most scenarios of the CTR benchmark. It is worth noting that the proposed method can recognize zero-shot Chinese characters in text images without fine-tuning, whereas previous methods require fine-tuning when new classes appear. The code is available at https://github.com/FudanVI/FudanOCR/tree/main/image-ids-CTR.","accessed":{"date-parts":[["2024",7,12]]},"author":[{"family":"Yu","given":"Haiyang"},{"family":"Wang","given":"Xiaocong"},{"family":"Li","given":"Bin"},{"family":"Xue","given":"Xiangyang"}],"citation-key":"ChineseTextRecognitionYu2309.01083","DOI":"10.48550/arXiv.2309.01083","issued":{"date-parts":[["2023",9,3]]},"number":"arXiv:2309.01083","publisher":"arXiv","source":"arXiv.org","title":"Chinese Text Recognition with A Pre-Trained CLIP-Like Model Through Image-IDS Aligning","type":"article","URL":"http://arxiv.org/abs/2309.01083"},
  {"id":"CogAgentHong2312.08914","abstract":"People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120*1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW, advancing the state of the art. The model and codes are available at https://github.com/THUDM/CogVLM .","accessed":{"date-parts":[["2024",4,10]]},"author":[{"family":"Hong","given":"Wenyi"},{"family":"Wang","given":"Weihan"},{"family":"Lv","given":"Qingsong"},{"family":"Xu","given":"Jiazheng"},{"family":"Yu","given":"Wenmeng"},{"family":"Ji","given":"Junhui"},{"family":"Wang","given":"Yan"},{"family":"Wang","given":"Zihan"},{"family":"Zhang","given":"Yuxuan"},{"family":"Li","given":"Juanzi"},{"family":"Xu","given":"Bin"},{"family":"Dong","given":"Yuxiao"},{"family":"Ding","given":"Ming"},{"family":"Tang","given":"Jie"}],"citation-key":"CogAgentHong2312.08914","DOI":"10.48550/arXiv.2312.08914","issued":{"date-parts":[["2023",12,21]]},"number":"arXiv:2312.08914","publisher":"arXiv","source":"arXiv.org","title":"CogAgent: A Visual Language Model for GUI Agents","title-short":"CogAgent","type":"article","URL":"http://arxiv.org/abs/2312.08914"},
  {"id":"DatasetAgnosticDocumentMondal2023","abstract":"Localizing document objects such as tables, figures, and equations is a primary step for extracting information from document images. We propose a novel end-to-end trainable deep network, termed Document Object Localization Network (dolnet), for detecting various objects present in the document images. The proposed network is a multi-stage extension of Mask r-cnn with a dual backbone having deformable convolution for detecting document objects with high detection accuracy at a higher IoU threshold. We also empirically evaluate the proposed dolnet on the publicly available benchmark datasets. The proposed DOLNet achieves state-of-the-art performance for most of the bench-mark datasets under various existing experimental environments. Our solution has three important properties: (i) a single trained model dolnet‡ that performs well across all the popular benchmark datasets, (ii) reports excellent performances across multiple, including with higher IoU thresholds, and (iii) consistently demonstrate the superior quantitative performance by following the same protocol of the recent works for each of the benchmarks.","accessed":{"date-parts":[["2024",7,12]]},"author":[{"family":"Mondal","given":"Ajoy"},{"family":"Agarwal","given":"Madhav"},{"family":"Jawahar","given":"C. V."}],"citation-key":"DatasetAgnosticDocumentMondal2023","container-title":"Pattern Recognition","container-title-short":"Pattern Recognition","DOI":"10.1016/j.patcog.2023.109698","ISSN":"0031-3203","issued":{"date-parts":[["2023",10,1]]},"page":"109698","source":"ScienceDirect","title":"Dataset agnostic document object detection","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0031320323003965","volume":"142"},
  {"id":"DocFormerAppalaraju2106.11539","abstract":"We present DocFormer -- a multi-modal transformer based architecture for the task of Visual Document Understanding (VDU). VDU is a challenging problem which aims to understand documents in their varied formats (forms, receipts etc.) and layouts. In addition, DocFormer is pre-trained in an unsupervised fashion using carefully designed tasks which encourage multi-modal interaction. DocFormer uses text, vision and spatial features and combines them using a novel multi-modal self-attention layer. DocFormer also shares learned spatial embeddings across modalities which makes it easy for the model to correlate text to visual tokens and vice versa. DocFormer is evaluated on 4 different datasets each with strong baselines. DocFormer achieves state-of-the-art results on all of them, sometimes beating models 4x its size (in no. of parameters).","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Appalaraju","given":"Srikar"},{"family":"Jasani","given":"Bhavan"},{"family":"Kota","given":"Bhargava Urala"},{"family":"Xie","given":"Yusheng"},{"family":"Manmatha","given":"R."}],"citation-key":"DocFormerAppalaraju2106.11539","DOI":"10.48550/arXiv.2106.11539","issued":{"date-parts":[["2021",9,20]]},"number":"arXiv:2106.11539","publisher":"arXiv","source":"arXiv.org","title":"DocFormer: End-to-End Transformer for Document Understanding","title-short":"DocFormer","type":"article","URL":"http://arxiv.org/abs/2106.11539"},
  {"id":"DocFormerv2Appalaraju2306.01733","abstract":"We propose DocFormerv2, a multi-modal transformer for Visual Document Understanding (VDU). The VDU domain entails understanding documents (beyond mere OCR predictions) e.g., extracting information from a form, VQA for documents and other tasks. VDU is challenging as it needs a model to make sense of multiple modalities (visual, language and spatial) to make a prediction. Our approach, termed DocFormerv2 is an encoder-decoder transformer which takes as input - vision, language and spatial features. DocFormerv2 is pre-trained with unsupervised tasks employed asymmetrically i.e., two novel document tasks on encoder and one on the auto-regressive decoder. The unsupervised tasks have been carefully designed to ensure that the pre-training encourages local-feature alignment between multiple modalities. DocFormerv2 when evaluated on nine datasets shows state-of-the-art performance over strong baselines e.g. TabFact (4.3%), InfoVQA (1.4%), FUNSD (1%). Furthermore, to show generalization capabilities, on three VQA tasks involving scene-text, Doc- Formerv2 outperforms previous comparably-sized models and even does better than much larger models (such as GIT2, PaLi and Flamingo) on some tasks. Extensive ablations show that due to its pre-training, DocFormerv2 understands multiple modalities better than prior-art in VDU.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Appalaraju","given":"Srikar"},{"family":"Tang","given":"Peng"},{"family":"Dong","given":"Qi"},{"family":"Sankaran","given":"Nishant"},{"family":"Zhou","given":"Yichu"},{"family":"Manmatha","given":"R."}],"citation-key":"DocFormerv2Appalaraju2306.01733","DOI":"10.48550/arXiv.2306.01733","issued":{"date-parts":[["2023",6,2]]},"number":"arXiv:2306.01733","publisher":"arXiv","source":"arXiv.org","title":"DocFormerv2: Local Features for Document Understanding","title-short":"DocFormerv2","type":"article","URL":"http://arxiv.org/abs/2306.01733"},
  {"id":"DocPediaFeng2311.11810","abstract":"This work presents DocPedia, a novel large multimodal model (LMM) for versatile OCR-free document understanding, capable of parsing images up to 2,560$\\times$2,560 resolution. Unlike existing work either struggle with high-resolution documents or give up the large language model thus vision or language ability constrained, our DocPedia directly processes visual input in the frequency domain rather than the pixel space. The unique characteristic enables DocPedia to capture a greater amount of visual and textual information using a limited number of visual tokens. To consistently enhance both perception and comprehension abilities of our model, we develop a dual-stage training strategy and enrich instructions/annotations of all training tasks covering multiple document types. Extensive quantitative and qualitative experiments conducted on various publicly available benchmarks confirm the mutual benefits of jointly learning perception and comprehension tasks. The results provide further evidence of the effectiveness and superior performance of our DocPedia over other methods.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Feng","given":"Hao"},{"family":"Liu","given":"Qi"},{"family":"Liu","given":"Hao"},{"family":"Zhou","given":"Wengang"},{"family":"Li","given":"Houqiang"},{"family":"Huang","given":"Can"}],"citation-key":"DocPediaFeng2311.11810","DOI":"10.48550/arXiv.2311.11810","issued":{"date-parts":[["2023",11,30]]},"number":"arXiv:2311.11810","publisher":"arXiv","source":"arXiv.org","title":"DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding","title-short":"DocPedia","type":"article","URL":"http://arxiv.org/abs/2311.11810"},
  {"id":"DocumentAICui2111.08609","abstract":"Document AI, or Document Intelligence, is a relatively new research topic that refers to the techniques for automatically reading, understanding, and analyzing business documents. It is an important research direction for natural language processing and computer vision. In recent years, the popularity of deep learning technology has greatly advanced the development of Document AI, such as document layout analysis, visual information extraction, document visual question answering, document image classification, etc. This paper briefly reviews some of the representative models, tasks, and benchmark datasets. Furthermore, we also introduce early-stage heuristic rule-based document analysis, statistical machine learning algorithms, and deep learning approaches especially pre-training methods. Finally, we look into future directions for Document AI research.","accessed":{"date-parts":[["2024",3,20]]},"author":[{"family":"Cui","given":"Lei"},{"family":"Xu","given":"Yiheng"},{"family":"Lv","given":"Tengchao"},{"family":"Wei","given":"Furu"}],"citation-key":"DocumentAICui2111.08609","DOI":"10.48550/arXiv.2111.08609","issued":{"date-parts":[["2021",11,16]]},"number":"arXiv:2111.08609","publisher":"arXiv","source":"arXiv.org","title":"Document AI: Benchmarks, Models and Applications","title-short":"Document AI","type":"article","URL":"http://arxiv.org/abs/2111.08609"},
  {"id":"DocumentUnderstandingDatasetVanLandeghem2305.08455","abstract":"We call on the Document AI (DocAI) community to reevaluate current methodologies and embrace the challenge of creating more practically-oriented benchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to remediate the halted research progress in understanding visually-rich documents (VRDs). We present a new dataset with novelties related to types of questions, answers, and document layouts based on multi-industry, multi-domain, and multi-page VRDs of various origins, and dates. Moreover, we are pushing the boundaries of current methods by creating multi-task and multi-domain evaluation setups that more accurately simulate real-world situations where powerful generalization and adaptation under low-resource settings are desired. DUDE aims to set a new standard as a more practical, long-standing benchmark for the community, and we hope that it will lead to future extensions and contributions that address real-world challenges. Finally, our work illustrates the importance of finding more efficient ways to model language, images, and layout in DocAI.","accessed":{"date-parts":[["2024",7,12]]},"author":[{"family":"Van Landeghem","given":"Jordy"},{"family":"Tito","given":"Rubén"},{"family":"Borchmann","given":"Łukasz"},{"family":"Pietruszka","given":"Michał"},{"family":"Józiak","given":"Paweł"},{"family":"Powalski","given":"Rafał"},{"family":"Jurkiewicz","given":"Dawid"},{"family":"Coustaty","given":"Mickaël"},{"family":"Ackaert","given":"Bertrand"},{"family":"Valveny","given":"Ernest"},{"family":"Blaschko","given":"Matthew"},{"family":"Moens","given":"Sien"},{"family":"Stanisławek","given":"Tomasz"}],"citation-key":"DocumentUnderstandingDatasetVanLandeghem2305.08455","DOI":"10.48550/arXiv.2305.08455","issued":{"date-parts":[["2023",9,11]]},"number":"arXiv:2305.08455","publisher":"arXiv","source":"arXiv.org","title":"Document Understanding Dataset and Evaluation (DUDE)","type":"article","URL":"http://arxiv.org/abs/2305.08455"},
  {"id":"EnhancedChartUnderstandingZhou2305.18641","abstract":"Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language(V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart understanding. We introduce ChartT5, a V+L model that learns how to interpret table information from chart images via cross-modal pre-training on plot table pairs. Specifically, we propose two novel pre-training objectives: Masked Header Prediction (MHP) and Masked Value Prediction (MVP) to facilitate the model with different skills to interpret the table information. We have conducted extensive experiments on chart question answering and chart summarization to verify the effectiveness of the proposed pre-training strategies. In particular, on the ChartQA benchmark, our ChartT5 outperforms the state-of-the-art non-pretraining methods by over 8% performance gains.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Zhou","given":"Mingyang"},{"family":"Fung","given":"Yi R."},{"family":"Chen","given":"Long"},{"family":"Thomas","given":"Christopher"},{"family":"Ji","given":"Heng"},{"family":"Chang","given":"Shih-Fu"}],"citation-key":"EnhancedChartUnderstandingZhou2305.18641","DOI":"10.48550/arXiv.2305.18641","issued":{"date-parts":[["2023",5,29]]},"number":"arXiv:2305.18641","publisher":"arXiv","source":"arXiv.org","title":"Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs","type":"article","URL":"http://arxiv.org/abs/2305.18641"},
  {"id":"FocusAnywhereFinegrainedLiu2405.14295","abstract":"Modern LVLMs still struggle to achieve fine-grained document understanding, such as OCR/translation/caption for regions of interest to the user, tasks that require the context of the entire page, or even multiple pages. Accordingly, this paper proposes Fox, an effective pipeline, hybrid data, and tuning strategy, that catalyzes LVLMs to focus anywhere on single/multi-page documents. We introduce a novel task to boost the document understanding by making LVLMs focus attention on the document-level region, such as redefining full-page OCR as foreground focus. We employ multiple vision vocabularies to extract visual hybrid knowledge for interleaved document pages (e.g., a page containing a photo). Meanwhile, we render cross-vocabulary vision data as the catalyzer to achieve a full reaction of multiple visual vocabularies and in-document figure understanding. Further, without modifying the weights of multiple vision vocabularies, the above catalyzed fine-grained understanding capabilities can be efficiently tuned to multi-page documents, enabling the model to focus anywhere in both format-free and page-free manners. Besides, we build a benchmark including 9 fine-grained sub-tasks (e.g., region-level OCR/summary, color-guided OCR) to promote document analysis in the community. The experimental results verify the superiority of our model.","accessed":{"date-parts":[["2024",6,6]]},"author":[{"family":"Liu","given":"Chenglong"},{"family":"Wei","given":"Haoran"},{"family":"Chen","given":"Jinyue"},{"family":"Kong","given":"Lingyu"},{"family":"Ge","given":"Zheng"},{"family":"Zhu","given":"Zining"},{"family":"Zhao","given":"Liang"},{"family":"Sun","given":"Jianjian"},{"family":"Han","given":"Chunrui"},{"family":"Zhang","given":"Xiangyu"}],"citation-key":"FocusAnywhereFinegrainedLiu2405.14295","DOI":"10.48550/arXiv.2405.14295","issued":{"date-parts":[["2024",5,23]]},"number":"arXiv:2405.14295","publisher":"arXiv","source":"arXiv.org","title":"Focus Anywhere for Fine-grained Multi-page Document Understanding","type":"article","URL":"http://arxiv.org/abs/2405.14295"},
  {"id":"FormNetLee2203.08411","abstract":"Sequence modeling has demonstrated state-of-the-art performance on natural language and document understanding tasks. However, it is challenging to correctly serialize tokens in form-like documents in practice due to their variety of layout patterns. We propose FormNet, a structure-aware sequence model to mitigate the suboptimal serialization of forms. First, we design Rich Attention that leverages the spatial relationship between tokens in a form for more precise attention score calculation. Second, we construct Super-Tokens for each word by embedding representations from their neighboring tokens through graph convolutions. FormNet therefore explicitly recovers local syntactic information that may have been lost during serialization. In experiments, FormNet outperforms existing methods with a more compact model size and less pre-training data, establishing new state-of-the-art performance on CORD, FUNSD and Payment benchmarks.","accessed":{"date-parts":[["2024",7,14]]},"author":[{"family":"Lee","given":"Chen-Yu"},{"family":"Li","given":"Chun-Liang"},{"family":"Dozat","given":"Timothy"},{"family":"Perot","given":"Vincent"},{"family":"Su","given":"Guolong"},{"family":"Hua","given":"Nan"},{"family":"Ainslie","given":"Joshua"},{"family":"Wang","given":"Renshen"},{"family":"Fujii","given":"Yasuhisa"},{"family":"Pfister","given":"Tomas"}],"citation-key":"FormNetLee2203.08411","DOI":"10.48550/arXiv.2203.08411","issued":{"date-parts":[["2022",3,23]]},"number":"arXiv:2203.08411","publisher":"arXiv","source":"arXiv.org","title":"FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction","title-short":"FormNet","type":"article","URL":"http://arxiv.org/abs/2203.08411"},
  {"id":"FormNetV2Lee2305.02549","abstract":"The recent advent of self-supervised pre-training techniques has led to a surge in the use of multimodal learning in form document understanding. However, existing approaches that extend the mask language modeling to other modalities require careful multi-task tuning, complex reconstruction target designs, or additional pre-training data. In FormNetV2, we introduce a centralized multimodal graph contrastive learning strategy to unify self-supervised pre-training for all modalities in one loss. The graph contrastive objective maximizes the agreement of multimodal representations, providing a natural interplay for all modalities without special customization. In addition, we extract image features within the bounding box that joins a pair of tokens connected by a graph edge, capturing more targeted visual cues without loading a sophisticated and separately pre-trained image embedder. FormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE and Payment benchmarks with a more compact model size.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Lee","given":"Chen-Yu"},{"family":"Li","given":"Chun-Liang"},{"family":"Zhang","given":"Hao"},{"family":"Dozat","given":"Timothy"},{"family":"Perot","given":"Vincent"},{"family":"Su","given":"Guolong"},{"family":"Zhang","given":"Xiang"},{"family":"Sohn","given":"Kihyuk"},{"family":"Glushnev","given":"Nikolai"},{"family":"Wang","given":"Renshen"},{"family":"Ainslie","given":"Joshua"},{"family":"Long","given":"Shangbang"},{"family":"Qin","given":"Siyang"},{"family":"Fujii","given":"Yasuhisa"},{"family":"Hua","given":"Nan"},{"family":"Pfister","given":"Tomas"}],"citation-key":"FormNetV2Lee2305.02549","DOI":"10.48550/arXiv.2305.02549","issued":{"date-parts":[["2023",6,13]]},"number":"arXiv:2305.02549","publisher":"arXiv","source":"arXiv.org","title":"FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction","title-short":"FormNetV2","type":"article","URL":"http://arxiv.org/abs/2305.02549"},
  {"id":"HiddenMysteryOCRLiu2305.07895","abstract":"Large models have recently played a dominant role in natural language processing and multimodal vision-language learning. However, their effectiveness in text-related visual tasks remains relatively unexplored. In this paper, we conducted a comprehensive evaluation of Large Multimodal Models, such as GPT4V and Gemini, in various text-related visual tasks including Text Recognition, Scene Text-Centric Visual Question Answering (VQA), Document-Oriented VQA, Key Information Extraction (KIE), and Handwritten Mathematical Expression Recognition (HMER). To facilitate the assessment of Optical Character Recognition (OCR) capabilities in Large Multimodal Models, we propose OCRBench, a comprehensive evaluation benchmark.Our study encompasses 29 datasets, making it the most comprehensive OCR evaluation benchmark available. Furthermore, our study reveals both the strengths and weaknesses of these models, particularly in handling multilingual text, handwritten text, non-semantic text, and mathematical expression recognition. Most importantly, the baseline results showcased in this study could provide a foundational framework for the conception and assessment of innovative strategies targeted at enhancing zero-shot multimodal techniques. The evaluation pipeline and benchmark are available at https://github.com/Yuliang-Liu/MultimodalOCR.","accessed":{"date-parts":[["2024",3,22]]},"author":[{"family":"Liu","given":"Yuliang"},{"family":"Li","given":"Zhang"},{"family":"Yang","given":"Biao"},{"family":"Li","given":"Chunyuan"},{"family":"Yin","given":"Xucheng"},{"family":"Liu","given":"Cheng-lin"},{"family":"Jin","given":"Lianwen"},{"family":"Bai","given":"Xiang"}],"citation-key":"HiddenMysteryOCRLiu2305.07895","issued":{"date-parts":[["2024",1,17]]},"number":"arXiv:2305.07895","publisher":"arXiv","source":"arXiv.org","title":"On the Hidden Mystery of OCR in Large Multimodal Models","type":"article","URL":"http://arxiv.org/abs/2305.07895"},
  {"id":"HierarchicalMultimodalTransformersTito2023","abstract":"Existing work on DocVQA only considers single-page documents. However, in real applications documents are mostly composed of multiple pages that should be processed altogether. In this work, we propose a new multimodal hierarchical method Hi-VT5, that overcomes the limitations of current methods to process long multipage documents. In contrast to previous hierarchical methods that focus on different semantic granularity (He et al., 2021) or different subtasks (Zhou et al., 2022) used in image classification. Our method is a hierarchical transformer architecture where the encoder learns to summarize the most relevant information of every page and then, the decoder uses this summarized representation to generate the final answer, following a bottom-up approach. Moreover, due to the lack of multipage DocVQA datasets, we also introduce MP-DocVQA, an extension of SP-DocVQA where questions are posed over multipage documents instead of single pages. Through extensive experimentation, we demonstrate that Hi-VT5 is able, in a single stage, to answer the questions and provide the page that contains the answer, which can be used as a kind of explainability measure.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Tito","given":"Rubèn"},{"family":"Karatzas","given":"Dimosthenis"},{"family":"Valveny","given":"Ernest"}],"citation-key":"HierarchicalMultimodalTransformersTito2023","container-title":"Pattern Recognition","container-title-short":"Pattern Recognition","DOI":"10.1016/j.patcog.2023.109834","ISSN":"0031-3203","issued":{"date-parts":[["2023",12,1]]},"page":"109834","source":"ScienceDirect","title":"Hierarchical multimodal transformers for Multipage DocVQA","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0031320323005320","volume":"144"},
  {"id":"HowFarAreWetoGPT-4V?Chen2024","abstract":"In this report, we introduce InternVL 1.5, an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. We introduce three simple improvements: (1) Strong Vision Encoder: we explored a continuous learning strategy for the large-scale vision foundation model -- InternViT-6B, boosting its visual understanding capabilities, and making it can be transferred and reused in different LLMs. (2) Dynamic High-Resolution: we divide images into tiles ranging from 1 to 40 of 448$\\times$448 pixels according to the aspect ratio and resolution of the input images, which supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we carefully collected a high-quality bilingual dataset that covers common scenes, document images, and annotated them with English and Chinese question-answer pairs, significantly enhancing performance in OCR- and Chinese-related tasks. We evaluate InternVL 1.5 through a series of benchmarks and comparative studies. Compared to both open-source and proprietary models, InternVL 1.5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks. Code has been released at https://github.com/OpenGVLab/InternVL.","accessed":{"date-parts":[["2024",6,6]]},"author":[{"family":"Chen","given":"Zhe"},{"family":"Wang","given":"Weiyun"},{"family":"Tian","given":"Hao"},{"family":"Ye","given":"Shenglong"},{"family":"Gao","given":"Zhangwei"},{"family":"Cui","given":"Erfei"},{"family":"Tong","given":"Wenwen"},{"family":"Hu","given":"Kongzhi"},{"family":"Luo","given":"Jiapeng"},{"family":"Ma","given":"Zheng"},{"family":"Ma","given":"Ji"},{"family":"Wang","given":"Jiaqi"},{"family":"Dong","given":"Xiaoyi"},{"family":"Yan","given":"Hang"},{"family":"Guo","given":"Hewei"},{"family":"He","given":"Conghui"},{"family":"Shi","given":"Botian"},{"family":"Jin","given":"Zhenjiang"},{"family":"Xu","given":"Chao"},{"family":"Wang","given":"Bin"},{"family":"Wei","given":"Xingjian"},{"family":"Li","given":"Wei"},{"family":"Zhang","given":"Wenjian"},{"family":"Zhang","given":"Bo"},{"family":"Cai","given":"Pinlong"},{"family":"Wen","given":"Licheng"},{"family":"Yan","given":"Xiangchao"},{"family":"Dou","given":"Min"},{"family":"Lu","given":"Lewei"},{"family":"Zhu","given":"Xizhou"},{"family":"Lu","given":"Tong"},{"family":"Lin","given":"Dahua"},{"family":"Qiao","given":"Yu"},{"family":"Dai","given":"Jifeng"},{"family":"Wang","given":"Wenhai"}],"citation-key":"HowFarAreWetoGPT-4V?Chen2024","container-title":"arXiv.org","issued":{"date-parts":[["2024",4,25]]},"language":"en","title":"How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites","title-short":"How Far Are We to GPT-4V?","type":"webpage","URL":"https://arxiv.org/abs/2404.16821v2"},
  {"id":"HRVDALiu2404.06918","abstract":"Leveraging vast training data, multimodal large language models (MLLMs) have demonstrated formidable general visual comprehension capabilities and achieved remarkable performance across various tasks. However, their performance in visual document understanding still leaves much room for improvement. This discrepancy is primarily attributed to the fact that visual document understanding is a fine-grained prediction task. In natural scenes, MLLMs typically use low-resolution images, leading to a substantial loss of visual information. Furthermore, general-purpose MLLMs do not excel in handling document-oriented instructions. In this paper, we propose a High-Resolution Visual Document Assistant (HRVDA), which bridges the gap between MLLMs and visual document understanding. This model employs a content filtering mechanism and an instruction filtering module to separately filter out the content-agnostic visual tokens and instruction-agnostic visual tokens, thereby achieving efficient model training and inference for high-resolution images. In addition, we construct a document-oriented visual instruction tuning dataset and apply a multi-stage training strategy to enhance the model's document modeling capabilities. Extensive experiments demonstrate that our model achieves state-of-the-art performance across multiple document understanding datasets, while maintaining training efficiency and inference speed comparable to low-resolution models.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Liu","given":"Chaohu"},{"family":"Yin","given":"Kun"},{"family":"Cao","given":"Haoyu"},{"family":"Jiang","given":"Xinghua"},{"family":"Li","given":"Xin"},{"family":"Liu","given":"Yinsong"},{"family":"Jiang","given":"Deqiang"},{"family":"Sun","given":"Xing"},{"family":"Xu","given":"Linli"}],"citation-key":"HRVDALiu2404.06918","DOI":"10.48550/arXiv.2404.06918","issued":{"date-parts":[["2024",4,10]]},"number":"arXiv:2404.06918","publisher":"arXiv","source":"arXiv.org","title":"HRVDA: High-Resolution Visual Document Assistant","title-short":"HRVDA","type":"article","URL":"http://arxiv.org/abs/2404.06918"},
  {"id":"InstructDocTanaka2401.13313","abstract":"We study the problem of completing various visual document understanding (VDU) tasks, e.g., question answering and information extraction, on real-world documents through human-written instructions. To this end, we propose InstructDoc, the first large-scale collection of 30 publicly available VDU datasets, each with diverse instructions in a unified format, which covers a wide range of 12 tasks and includes open document types/formats. Furthermore, to enhance the generalization performance on VDU tasks, we design a new instruction-based document reading and understanding model, InstructDr, that connects document images, image encoders, and large language models (LLMs) through a trainable bridging module. Experiments demonstrate that InstructDr can effectively adapt to new VDU datasets, tasks, and domains via given instructions and outperforms existing multimodal LLMs and ChatGPT without specific training.","accessed":{"date-parts":[["2024",7,12]]},"author":[{"family":"Tanaka","given":"Ryota"},{"family":"Iki","given":"Taichi"},{"family":"Nishida","given":"Kyosuke"},{"family":"Saito","given":"Kuniko"},{"family":"Suzuki","given":"Jun"}],"citation-key":"InstructDocTanaka2401.13313","DOI":"10.48550/arXiv.2401.13313","issued":{"date-parts":[["2024",1,24]]},"number":"arXiv:2401.13313","publisher":"arXiv","source":"arXiv.org","title":"InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions","title-short":"InstructDoc","type":"article","URL":"http://arxiv.org/abs/2401.13313"},
  {"id":"InternLM-XComposer-2.5Zhang2407.03320","abstract":"We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts. Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks. The InternLM-XComposer-2.5 is publicly available at https://github.com/InternLM/InternLM-XComposer.","accessed":{"date-parts":[["2024",7,15]]},"author":[{"family":"Zhang","given":"Pan"},{"family":"Dong","given":"Xiaoyi"},{"family":"Zang","given":"Yuhang"},{"family":"Cao","given":"Yuhang"},{"family":"Qian","given":"Rui"},{"family":"Chen","given":"Lin"},{"family":"Guo","given":"Qipeng"},{"family":"Duan","given":"Haodong"},{"family":"Wang","given":"Bin"},{"family":"Ouyang","given":"Linke"},{"family":"Zhang","given":"Songyang"},{"family":"Zhang","given":"Wenwei"},{"family":"Li","given":"Yining"},{"family":"Gao","given":"Yang"},{"family":"Sun","given":"Peng"},{"family":"Zhang","given":"Xinyue"},{"family":"Li","given":"Wei"},{"family":"Li","given":"Jingwen"},{"family":"Wang","given":"Wenhai"},{"family":"Yan","given":"Hang"},{"family":"He","given":"Conghui"},{"family":"Zhang","given":"Xingcheng"},{"family":"Chen","given":"Kai"},{"family":"Dai","given":"Jifeng"},{"family":"Qiao","given":"Yu"},{"family":"Lin","given":"Dahua"},{"family":"Wang","given":"Jiaqi"}],"citation-key":"InternLM-XComposer-2.5Zhang2407.03320","DOI":"10.48550/arXiv.2407.03320","issued":{"date-parts":[["2024",7,3]]},"number":"arXiv:2407.03320","publisher":"arXiv","source":"arXiv.org","title":"InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output","title-short":"InternLM-XComposer-2.5","type":"article","URL":"http://arxiv.org/abs/2407.03320"},
  {"id":"InternLM-XComposer2-4KHDDong2404.06512","abstract":"The Large Vision-Language Model (LVLM) field has seen significant advancements, yet its progression has been hindered by challenges in comprehending fine-grained visual content due to limited resolution. Recent efforts have aimed to enhance the high-resolution understanding capabilities of LVLMs, yet they remain capped at approximately 1500 x 1500 pixels and constrained to a relatively narrow resolution range. This paper represents InternLM-XComposer2-4KHD, a groundbreaking exploration into elevating LVLM resolution capabilities up to 4K HD (3840 x 1600) and beyond. Concurrently, considering the ultra-high resolution may not be necessary in all scenarios, it supports a wide range of diverse resolutions from 336 pixels to 4K standard, significantly broadening its scope of applicability. Specifically, this research advances the patch division paradigm by introducing a novel extension: dynamic resolution with automatic patch configuration. It maintains the training image aspect ratios while automatically varying patch counts and configuring layouts based on a pre-trained Vision Transformer (ViT) (336 x 336), leading to dynamic training resolution from 336 pixels to 4K standard. Our research demonstrates that scaling training resolution up to 4K HD leads to consistent performance enhancements without hitting the ceiling of potential improvements. InternLM-XComposer2-4KHD shows superb capability that matches or even surpasses GPT-4V and Gemini Pro in 10 of the 16 benchmarks. The InternLM-XComposer2-4KHD model series with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer.","accessed":{"date-parts":[["2024",6,20]]},"author":[{"family":"Dong","given":"Xiaoyi"},{"family":"Zhang","given":"Pan"},{"family":"Zang","given":"Yuhang"},{"family":"Cao","given":"Yuhang"},{"family":"Wang","given":"Bin"},{"family":"Ouyang","given":"Linke"},{"family":"Zhang","given":"Songyang"},{"family":"Duan","given":"Haodong"},{"family":"Zhang","given":"Wenwei"},{"family":"Li","given":"Yining"},{"family":"Yan","given":"Hang"},{"family":"Gao","given":"Yang"},{"family":"Chen","given":"Zhe"},{"family":"Zhang","given":"Xinyue"},{"family":"Li","given":"Wei"},{"family":"Li","given":"Jingwen"},{"family":"Wang","given":"Wenhai"},{"family":"Chen","given":"Kai"},{"family":"He","given":"Conghui"},{"family":"Zhang","given":"Xingcheng"},{"family":"Dai","given":"Jifeng"},{"family":"Qiao","given":"Yu"},{"family":"Lin","given":"Dahua"},{"family":"Wang","given":"Jiaqi"}],"citation-key":"InternLM-XComposer2-4KHDDong2404.06512","DOI":"10.48550/arXiv.2404.06512","issued":{"date-parts":[["2024",4,9]]},"number":"arXiv:2404.06512","publisher":"arXiv","source":"arXiv.org","title":"InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD","title-short":"InternLM-XComposer2-4KHD","type":"article","URL":"http://arxiv.org/abs/2404.06512"},
  {"id":"InternLM-XComposer2Dong2401.16420","abstract":"We introduce InternLM-XComposer2, a cutting-edge vision-language model excelling in free-form text-image composition and comprehension. This model goes beyond conventional vision-language understanding, adeptly crafting interleaved text-image content from diverse inputs like outlines, detailed textual specifications, and reference images, enabling highly customizable content creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach that applies additional LoRA parameters exclusively to image tokens to preserve the integrity of pre-trained language knowledge, striking a balance between precise vision understanding and text composition with literary talent. Experimental results demonstrate the superiority of InternLM-XComposer2 based on InternLM2-7B in producing high-quality long-text multi-modal content and its exceptional vision-language understanding performance across various benchmarks, where it not only significantly outperforms existing multimodal models but also matches or even surpasses GPT-4V and Gemini Pro in certain assessments. This highlights its remarkable proficiency in the realm of multimodal understanding. The InternLM-XComposer2 model series with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer.","accessed":{"date-parts":[["2024",6,20]]},"author":[{"family":"Dong","given":"Xiaoyi"},{"family":"Zhang","given":"Pan"},{"family":"Zang","given":"Yuhang"},{"family":"Cao","given":"Yuhang"},{"family":"Wang","given":"Bin"},{"family":"Ouyang","given":"Linke"},{"family":"Wei","given":"Xilin"},{"family":"Zhang","given":"Songyang"},{"family":"Duan","given":"Haodong"},{"family":"Cao","given":"Maosong"},{"family":"Zhang","given":"Wenwei"},{"family":"Li","given":"Yining"},{"family":"Yan","given":"Hang"},{"family":"Gao","given":"Yang"},{"family":"Zhang","given":"Xinyue"},{"family":"Li","given":"Wei"},{"family":"Li","given":"Jingwen"},{"family":"Chen","given":"Kai"},{"family":"He","given":"Conghui"},{"family":"Zhang","given":"Xingcheng"},{"family":"Qiao","given":"Yu"},{"family":"Lin","given":"Dahua"},{"family":"Wang","given":"Jiaqi"}],"citation-key":"InternLM-XComposer2Dong2401.16420","DOI":"10.48550/arXiv.2401.16420","issued":{"date-parts":[["2024",1,29]]},"number":"arXiv:2401.16420","publisher":"arXiv","source":"arXiv.org","title":"InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model","title-short":"InternLM-XComposer2","type":"article","URL":"http://arxiv.org/abs/2401.16420"},
  {"id":"Kosmos-2.5Lv2309.11419","abstract":"We present Kosmos-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format. This unified multimodal literate capability is achieved through a shared Transformer architecture, task-specific prompts, and flexible text representations. We evaluate Kosmos-2.5 on end-to-end document-level text recognition and image-to-markdown text generation. Furthermore, the model can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images. This work also paves the way for the future scaling of multimodal large language models.","accessed":{"date-parts":[["2024",7,15]]},"author":[{"family":"Lv","given":"Tengchao"},{"family":"Huang","given":"Yupan"},{"family":"Chen","given":"Jingye"},{"family":"Cui","given":"Lei"},{"family":"Ma","given":"Shuming"},{"family":"Chang","given":"Yaoyao"},{"family":"Huang","given":"Shaohan"},{"family":"Wang","given":"Wenhui"},{"family":"Dong","given":"Li"},{"family":"Luo","given":"Weiyao"},{"family":"Wu","given":"Shaoxiang"},{"family":"Wang","given":"Guoxin"},{"family":"Zhang","given":"Cha"},{"family":"Wei","given":"Furu"}],"citation-key":"Kosmos-2.5Lv2309.11419","DOI":"10.48550/arXiv.2309.11419","issued":{"date-parts":[["2023",9,20]]},"number":"arXiv:2309.11419","publisher":"arXiv","source":"arXiv.org","title":"Kosmos-2.5: A Multimodal Literate Model","title-short":"Kosmos-2.5","type":"article","URL":"http://arxiv.org/abs/2309.11419"},
  {"id":"LayoutLLMLuo2404.05225","abstract":"Recently, leveraging large language models (LLMs) or multimodal large language models (MLLMs) for document understanding has been proven very promising. However, previous works that employ LLMs/MLLMs for document understanding have not fully explored and utilized the document layout information, which is vital for precise document understanding. In this paper, we propose LayoutLLM, an LLM/MLLM based method for document understanding. The core of LayoutLLM is a layout instruction tuning strategy, which is specially designed to enhance the comprehension and utilization of document layouts. The proposed layout instruction tuning strategy consists of two components: Layout-aware Pre-training and Layout-aware Supervised Fine-tuning. To capture the characteristics of document layout in Layout-aware Pre-training, three groups of pre-training tasks, corresponding to document-level, region-level and segment-level information, are introduced. Furthermore, a novel module called layout chain-of-thought (LayoutCoT) is devised to enable LayoutLLM to focus on regions relevant to the question and generate accurate answers. LayoutCoT is effective for boosting the performance of document understanding. Meanwhile, it brings a certain degree of interpretability, which could facilitate manual inspection and correction. Experiments on standard benchmarks show that the proposed LayoutLLM significantly outperforms existing methods that adopt open-source 7B LLMs/MLLMs for document understanding. The training data of the LayoutLLM is publicly available at https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LayoutLLM","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Luo","given":"Chuwei"},{"family":"Shen","given":"Yufan"},{"family":"Zhu","given":"Zhaoqing"},{"family":"Zheng","given":"Qi"},{"family":"Yu","given":"Zhi"},{"family":"Yao","given":"Cong"}],"citation-key":"LayoutLLMLuo2404.05225","DOI":"10.48550/arXiv.2404.05225","issued":{"date-parts":[["2024",4,8]]},"number":"arXiv:2404.05225","publisher":"arXiv","source":"arXiv.org","title":"LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding","title-short":"LayoutLLM","type":"article","URL":"http://arxiv.org/abs/2404.05225"},
  {"id":"LayoutLMv2Xu2012.14740","abstract":"Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 $\\to$ 0.8420), CORD (0.9493 $\\to$ 0.9601), SROIE (0.9524 $\\to$ 0.9781), Kleister-NDA (0.8340 $\\to$ 0.8520), RVL-CDIP (0.9443 $\\to$ 0.9564), and DocVQA (0.7295 $\\to$ 0.8672). We made our model and code publicly available at \\url{https://aka.ms/layoutlmv2}.","accessed":{"date-parts":[["2024",3,21]]},"author":[{"family":"Xu","given":"Yang"},{"family":"Xu","given":"Yiheng"},{"family":"Lv","given":"Tengchao"},{"family":"Cui","given":"Lei"},{"family":"Wei","given":"Furu"},{"family":"Wang","given":"Guoxin"},{"family":"Lu","given":"Yijuan"},{"family":"Florencio","given":"Dinei"},{"family":"Zhang","given":"Cha"},{"family":"Che","given":"Wanxiang"},{"family":"Zhang","given":"Min"},{"family":"Zhou","given":"Lidong"}],"citation-key":"LayoutLMv2Xu2012.14740","issued":{"date-parts":[["2022",1,9]]},"number":"arXiv:2012.14740","publisher":"arXiv","source":"arXiv.org","title":"LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding","title-short":"LayoutLMv2","type":"article","URL":"http://arxiv.org/abs/2012.14740","version":"4"},
  {"id":"LayoutLMv3Huang2204.08387","abstract":"Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose \\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at \\url{https://aka.ms/layoutlmv3}.","accessed":{"date-parts":[["2024",7,12]]},"author":[{"family":"Huang","given":"Yupan"},{"family":"Lv","given":"Tengchao"},{"family":"Cui","given":"Lei"},{"family":"Lu","given":"Yutong"},{"family":"Wei","given":"Furu"}],"citation-key":"LayoutLMv3Huang2204.08387","DOI":"10.48550/arXiv.2204.08387","issued":{"date-parts":[["2022",7,19]]},"number":"arXiv:2204.08387","publisher":"arXiv","source":"arXiv.org","title":"LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking","title-short":"LayoutLMv3","type":"article","URL":"http://arxiv.org/abs/2204.08387"},
  {"id":"LayoutLMXu2020","abstract":"Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the \\textbf{LayoutLM} to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at \\url{https://aka.ms/layoutlm}.","accessed":{"date-parts":[["2024",3,20]]},"author":[{"family":"Xu","given":"Yiheng"},{"family":"Li","given":"Minghao"},{"family":"Cui","given":"Lei"},{"family":"Huang","given":"Shaohan"},{"family":"Wei","given":"Furu"},{"family":"Zhou","given":"Ming"}],"citation-key":"LayoutLMXu2020","container-title":"Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining","DOI":"10.1145/3394486.3403172","issued":{"date-parts":[["2020",8,23]]},"page":"1192-1200","source":"arXiv.org","title":"LayoutLM: Pre-training of Text and Layout for Document Image Understanding","title-short":"LayoutLM","type":"paper-conference","URL":"http://arxiv.org/abs/1912.13318"},
  {"id":"LayoutPointerSiyuan2024","abstract":"Visual Information Extraction (VIE), as a crucial task of Document Intelligence, involves two primary sub-tasks: Semantic Entity Recognition (SER) and Relation Extraction (RE). However, VIE faces two significant challenges. Firstly, most existing models inadequately utilize spatial information of entities, often failing to predict connections or incorrectly linking spatially distant entities. Secondly, the improper input order of tokens challenges in extracting complete entity pairs from documents with multi-line entities when text is extracted via PDF parser or OCR. To address these challenges, we propose LayoutPointer, a Spatial-Context Adaptive Pointer Network. LayoutPointer explicitly enhances spatial-context relationships by incorporating 2D relative position information and adaptive spatial constraints within self-attention. Furthermore, we recast the RE task as a specialized cycle detection problem, employing a unique tail-to-head pointer to restore the semantic order among multi-line entities. To better evaluate the effectiveness of our proposed method, we reconstruct a multi-line dataset named MLFUD, which more accurately reflects real-world scenarios. Fine-tuning experimental results on FUNSD, XFUND, and MLFUD datasets demonstrate that LayoutPointer significantly outperforms existing state-of-the-art methods in F1 scores for RE tasks (e.g., 5.71% improvement on XFUND using LayoutPointer_\\textBASE-X over LayoutLMv3).","accessed":{"date-parts":[["2024",7,12]]},"author":[{"family":"Siyuan","given":"Huang"},{"family":"Xiong","given":"Yongping"},{"family":"Guibin","given":"Wu"}],"citation-key":"LayoutPointerSiyuan2024","container-title":"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)","editor":[{"family":"Duh","given":"Kevin"},{"family":"Gomez","given":"Helena"},{"family":"Bethard","given":"Steven"}],"event-place":"Mexico City, Mexico","event-title":"NAACL-HLT 2024","issued":{"date-parts":[["2024",6]]},"page":"3737–3748","publisher":"Association for Computational Linguistics","publisher-place":"Mexico City, Mexico","source":"ACLWeb","title":"LayoutPointer: A Spatial-Context Adaptive Pointer Network for Visual Information Extraction","title-short":"LayoutPointer","type":"paper-conference","URL":"https://aclanthology.org/2024.naacl-long.207"},
  {"id":"LayoutXLMXu2104.08836","abstract":"Multimodal pre-training with text, layout, and image has achieved SOTA performance for visually-rich document understanding tasks recently, which demonstrates the great potential for joint learning across different modalities. In this paper, we present LayoutXLM, a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually-rich document understanding. To accurately evaluate LayoutXLM, we also introduce a multilingual form understanding benchmark dataset named XFUND, which includes form understanding samples in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and key-value pairs are manually labeled for each language. Experiment results show that the LayoutXLM model has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND dataset. The pre-trained LayoutXLM model and the XFUND dataset are publicly available at https://aka.ms/layoutxlm.","accessed":{"date-parts":[["2024",3,21]]},"author":[{"family":"Xu","given":"Yiheng"},{"family":"Lv","given":"Tengchao"},{"family":"Cui","given":"Lei"},{"family":"Wang","given":"Guoxin"},{"family":"Lu","given":"Yijuan"},{"family":"Florencio","given":"Dinei"},{"family":"Zhang","given":"Cha"},{"family":"Wei","given":"Furu"}],"citation-key":"LayoutXLMXu2104.08836","DOI":"10.48550/arXiv.2104.08836","issued":{"date-parts":[["2021",9,9]]},"number":"arXiv:2104.08836","publisher":"arXiv","source":"arXiv.org","title":"LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding","title-short":"LayoutXLM","type":"article","URL":"http://arxiv.org/abs/2104.08836"},
  {"id":"LLaVA-UHDXu2403.11703","abstract":"Visual encoding constitutes the basis of large multimodal models (LMMs) in understanding the visual world. Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness. In this work, we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy. To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution. LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for LLMs. Comprehensive experiments show that LLaVA-UHD outperforms established LMMs trained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our model built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088) resolution images using only 94% inference computation, and achieves 6.4 accuracy improvement on TextVQA. Moreover, the model can be efficiently trained in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of LLaVA-1.5). We make the data and code publicly available at https://github.com/thunlp/LLaVA-UHD.","accessed":{"date-parts":[["2024",6,20]]},"author":[{"family":"Xu","given":"Ruyi"},{"family":"Yao","given":"Yuan"},{"family":"Guo","given":"Zonghao"},{"family":"Cui","given":"Junbo"},{"family":"Ni","given":"Zanlin"},{"family":"Ge","given":"Chunjiang"},{"family":"Chua","given":"Tat-Seng"},{"family":"Liu","given":"Zhiyuan"},{"family":"Sun","given":"Maosong"},{"family":"Huang","given":"Gao"}],"citation-key":"LLaVA-UHDXu2403.11703","DOI":"10.48550/arXiv.2403.11703","issued":{"date-parts":[["2024",3,18]]},"number":"arXiv:2403.11703","publisher":"arXiv","source":"arXiv.org","title":"LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images","title-short":"LLaVA-UHD","type":"article","URL":"http://arxiv.org/abs/2403.11703"},
  {"id":"LLaVARZhang2306.17107","abstract":"Instruction tuning unlocks the superior capability of Large Language Models (LLM) to interact with humans. Furthermore, recent instruction-following datasets include images as visual inputs, collecting responses for image-based instructions. However, visual instruction-tuned models cannot comprehend textual details within images well. This work enhances the current visual instruction tuning pipeline with text-rich images (e.g., movie posters, book covers, etc.). Specifically, we first use publicly available OCR tools to collect results on 422K text-rich images from the LAION dataset. Moreover, we prompt text-only GPT-4 with recognized texts and image captions to generate 16K conversations, each containing question-answer pairs for text-rich images. By combining our collected data with previous multi-modal instruction-following data, our model, LLaVAR, substantially improves the LLaVA model's capability on text-based VQA datasets (up to 20% accuracy improvement) while achieving an accuracy of 91.42% on ScienceQA. The GPT-4-based instruction-following evaluation also demonstrates the improvement of our model on both natural images and text-rich images. Through qualitative analysis, LLaVAR shows promising interaction (e.g., reasoning, writing, and elaboration) skills with humans based on the latest real-world online content that combines text and images. We make our code/data/models publicly available at https://llavar.github.io/.","accessed":{"date-parts":[["2024",7,14]]},"author":[{"family":"Zhang","given":"Yanzhe"},{"family":"Zhang","given":"Ruiyi"},{"family":"Gu","given":"Jiuxiang"},{"family":"Zhou","given":"Yufan"},{"family":"Lipka","given":"Nedim"},{"family":"Yang","given":"Diyi"},{"family":"Sun","given":"Tong"}],"citation-key":"LLaVARZhang2306.17107","DOI":"10.48550/arXiv.2306.17107","issued":{"date-parts":[["2024",2,2]]},"number":"arXiv:2306.17107","publisher":"arXiv","source":"arXiv.org","title":"LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding","title-short":"LLaVAR","type":"article","URL":"http://arxiv.org/abs/2306.17107"},
  {"id":"LOCRSun2403.02127","abstract":"Academic documents are packed with texts, equations, tables, and figures, requiring comprehensive understanding for accurate Optical Character Recognition (OCR). While end-to-end OCR methods offer improved accuracy over layout-based approaches, they often grapple with significant repetition issues, especially with complex layouts in Out-Of-Domain (OOD) documents.To tackle this issue, we propose LOCR, a model that integrates location guiding into the transformer architecture during autoregression. We train the model on a dataset comprising over 77M text-location pairs from 125K academic document pages, including bounding boxes for words, tables and mathematical symbols. LOCR adeptly handles various formatting elements and generates content in Markdown language. It outperforms all existing methods in our test set constructed from arXiv, as measured by edit distance, BLEU, METEOR and F-measure.LOCR also reduces repetition frequency from 4.4% of pages to 0.5% in the arXiv dataset, from 13.2% to 1.3% in OOD quantum physics documents and from 8.1% to 1.8% in OOD marketing documents. Additionally, LOCR features an interactive OCR mode, facilitating the generation of complex documents through a few location prompts from human.","accessed":{"date-parts":[["2024",4,10]]},"author":[{"family":"Sun","given":"Yu"},{"family":"Zhou","given":"Dongzhan"},{"family":"Lin","given":"Chen"},{"family":"He","given":"Conghui"},{"family":"Ouyang","given":"Wanli"},{"family":"Zhong","given":"Han-Sen"}],"citation-key":"LOCRSun2403.02127","DOI":"10.48550/arXiv.2403.02127","issued":{"date-parts":[["2024",3,4]]},"number":"arXiv:2403.02127","publisher":"arXiv","source":"arXiv.org","title":"LOCR: Location-Guided Transformer for Optical Character Recognition","title-short":"LOCR","type":"article","URL":"http://arxiv.org/abs/2403.02127"},
  {"id":"MarkupLMLi2110.08518","abstract":"Multimodal pre-training with text, layout, and image has made significant progress for Visually Rich Document Understanding (VRDU), especially the fixed-layout documents such as scanned document images. While, there are still a large number of digital documents where the layout information is not fixed and needs to be interactively and dynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this paper, we propose MarkupLM for document understanding tasks with markup languages as the backbone, such as HTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the pre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding tasks. The pre-trained model and code will be publicly available at https://aka.ms/markuplm.","accessed":{"date-parts":[["2024",3,21]]},"author":[{"family":"Li","given":"Junlong"},{"family":"Xu","given":"Yiheng"},{"family":"Cui","given":"Lei"},{"family":"Wei","given":"Furu"}],"citation-key":"MarkupLMLi2110.08518","issued":{"date-parts":[["2022",3,11]]},"number":"arXiv:2110.08518","publisher":"arXiv","source":"arXiv.org","title":"MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding","title-short":"MarkupLM","type":"article","URL":"http://arxiv.org/abs/2110.08518","version":"2"},
  {"id":"MatChaLiu2212.09662","abstract":"Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models' capabilities in jointly modeling charts/plots and language data. Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MatCha pretraining starting from Pix2Struct, a recently proposed image-to-text visual language model. On standard benchmarks such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MatCha pretraining transfers to domains such as screenshots, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MatCha pretraining on broader visual language tasks.","accessed":{"date-parts":[["2024",7,12]]},"author":[{"family":"Liu","given":"Fangyu"},{"family":"Piccinno","given":"Francesco"},{"family":"Krichene","given":"Syrine"},{"family":"Pang","given":"Chenxi"},{"family":"Lee","given":"Kenton"},{"family":"Joshi","given":"Mandar"},{"family":"Altun","given":"Yasemin"},{"family":"Collier","given":"Nigel"},{"family":"Eisenschlos","given":"Julian Martin"}],"citation-key":"MatChaLiu2212.09662","DOI":"10.48550/arXiv.2212.09662","issued":{"date-parts":[["2023",5,23]]},"number":"arXiv:2212.09662","publisher":"arXiv","source":"arXiv.org","title":"MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering","title-short":"MatCha","type":"article","URL":"http://arxiv.org/abs/2212.09662"},
  {"id":"MonkeyLi2311.06607","abstract":"Large Multimodal Models (LMMs) have shown promise in vision-language tasks but struggle with high-resolution input and detailed scene understanding. Addressing these challenges, we introduce Monkey to enhance LMM capabilities. Firstly, Monkey processes input images by dividing them into uniform patches, each matching the size (e.g., 448x448) used in the original training of the well-trained vision encoder. Equipped with individual adapter for each patch, Monkey can handle higher resolutions up to 1344x896 pixels, enabling the detailed capture of complex visual information. Secondly, it employs a multi-level description generation method, enriching the context for scene-object associations. This two-part strategy ensures more effective learning from generated data: the higher resolution allows for a more detailed capture of visuals, which in turn enhances the effectiveness of comprehensive descriptions. Extensive ablative results validate the effectiveness of our designs. Additionally, experiments on 18 datasets further demonstrate that Monkey surpasses existing LMMs in many tasks like Image Captioning and various Visual Question Answering formats. Specially, in qualitative tests focused on dense text question answering, Monkey has exhibited encouraging results compared with GPT4V. Code is available at https://github.com/Yuliang-Liu/Monkey.","accessed":{"date-parts":[["2024",7,15]]},"author":[{"family":"Li","given":"Zhang"},{"family":"Yang","given":"Biao"},{"family":"Liu","given":"Qiang"},{"family":"Ma","given":"Zhiyin"},{"family":"Zhang","given":"Shuo"},{"family":"Yang","given":"Jingxu"},{"family":"Sun","given":"Yabo"},{"family":"Liu","given":"Yuliang"},{"family":"Bai","given":"Xiang"}],"citation-key":"MonkeyLi2311.06607","DOI":"10.48550/arXiv.2311.06607","issued":{"date-parts":[["2024",2,22]]},"number":"arXiv:2311.06607","publisher":"arXiv","source":"arXiv.org","title":"Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models","title-short":"Monkey","type":"article","URL":"http://arxiv.org/abs/2311.06607"},
  {"id":"mPLUG-DocOwl1.5Hu2403.12895","abstract":"Structure information is critical for understanding the semantics of text-rich images, such as documents, tables, and charts. Existing Multimodal Large Language Models (MLLMs) for Visual Document Understanding are equipped with text recognition ability but lack general structure understanding abilities for text-rich document images. In this work, we emphasize the importance of structure information in Visual Document Understanding and propose the Unified Structure Learning to boost the performance of MLLMs. Our Unified Structure Learning comprises structure-aware parsing tasks and multi-grained text localization tasks across 5 domains: document, webpage, table, chart, and natural image. To better encode structure information, we design a simple and effective vision-to-text module H-Reducer, which can not only maintain the layout information but also reduce the length of visual features by merging horizontal adjacent patches through convolution, enabling the LLM to understand high-resolution images more efficiently. Furthermore, by constructing structure-aware text sequences and multi-grained pairs of texts and bounding boxes for publicly available text-rich images, we build a comprehensive training set DocStruct4M to support structure learning. Finally, we construct a small but high-quality reasoning tuning dataset DocReason25K to trigger the detailed explanation ability in the document domain. Our model DocOwl 1.5 achieves state-of-the-art performance on 10 visual document understanding benchmarks, improving the SOTA performance of MLLMs with a 7B LLM by more than 10 points in 5/10 benchmarks. Our codes, models, and datasets are publicly available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Hu","given":"Anwen"},{"family":"Xu","given":"Haiyang"},{"family":"Ye","given":"Jiabo"},{"family":"Yan","given":"Ming"},{"family":"Zhang","given":"Liang"},{"family":"Zhang","given":"Bo"},{"family":"Li","given":"Chen"},{"family":"Zhang","given":"Ji"},{"family":"Jin","given":"Qin"},{"family":"Huang","given":"Fei"},{"family":"Zhou","given":"Jingren"}],"citation-key":"mPLUG-DocOwl1.5Hu2403.12895","DOI":"10.48550/arXiv.2403.12895","issued":{"date-parts":[["2024",3,19]]},"number":"arXiv:2403.12895","publisher":"arXiv","source":"arXiv.org","title":"mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding","title-short":"mPLUG-DocOwl 1.5","type":"article","URL":"http://arxiv.org/abs/2403.12895"},
  {"id":"mPLUG-DocOwlYe2307.02499","abstract":"Document understanding refers to automatically extract, analyze and comprehend information from various types of digital documents, such as a web page. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl, have demonstrated promising zero-shot capabilities in shallow OCR-free text recognition, indicating their potential for OCR-free document understanding. Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tables or large blocks of text, which are essential for OCR-free document understanding. In this paper, we propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding. Specifically, we first construct a instruction tuning dataset featuring a wide range of visual-text understanding tasks. Then, we strengthen the OCR-free document understanding ability by jointly train the model on language-only, general vision-and-language, and document instruction tuning dataset with our unified instruction tuning strategy. We also build an OCR-free document instruction understanding evaluation set LLMDoc to better compare models' capabilities on instruct compliance and document understanding. Experimental results show that our model outperforms existing multi-modal models, demonstrating its strong ability of document understanding. Besides, without specific fine-tuning, mPLUG-DocOwl generalizes well on various downstream tasks. Our code, models, training data and evaluation set are available at https://github.com/X-PLUG/mPLUG-DocOwl.","accessed":{"date-parts":[["2024",7,12]]},"author":[{"family":"Ye","given":"Jiabo"},{"family":"Hu","given":"Anwen"},{"family":"Xu","given":"Haiyang"},{"family":"Ye","given":"Qinghao"},{"family":"Yan","given":"Ming"},{"family":"Dan","given":"Yuhao"},{"family":"Zhao","given":"Chenlin"},{"family":"Xu","given":"Guohai"},{"family":"Li","given":"Chenliang"},{"family":"Tian","given":"Junfeng"},{"family":"Qi","given":"Qian"},{"family":"Zhang","given":"Ji"},{"family":"Huang","given":"Fei"}],"citation-key":"mPLUG-DocOwlYe2307.02499","DOI":"10.48550/arXiv.2307.02499","issued":{"date-parts":[["2023",7,4]]},"number":"arXiv:2307.02499","publisher":"arXiv","source":"arXiv.org","title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding","title-short":"mPLUG-DocOwl","type":"article","URL":"http://arxiv.org/abs/2307.02499"},
  {"id":"mPLUG-DocOwlYe2307.02499a","abstract":"Document understanding refers to automatically extract, analyze and comprehend information from various types of digital documents, such as a web page. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl, have demonstrated promising zero-shot capabilities in shallow OCR-free text recognition, indicating their potential for OCR-free document understanding. Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tables or large blocks of text, which are essential for OCR-free document understanding. In this paper, we propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding. Specifically, we first construct a instruction tuning dataset featuring a wide range of visual-text understanding tasks. Then, we strengthen the OCR-free document understanding ability by jointly train the model on language-only, general vision-and-language, and document instruction tuning dataset with our unified instruction tuning strategy. We also build an OCR-free document instruction understanding evaluation set LLMDoc to better compare models' capabilities on instruct compliance and document understanding. Experimental results show that our model outperforms existing multi-modal models, demonstrating its strong ability of document understanding. Besides, without specific fine-tuning, mPLUG-DocOwl generalizes well on various downstream tasks. Our code, models, training data and evaluation set are available at https://github.com/X-PLUG/mPLUG-DocOwl.","accessed":{"date-parts":[["2024",7,14]]},"author":[{"family":"Ye","given":"Jiabo"},{"family":"Hu","given":"Anwen"},{"family":"Xu","given":"Haiyang"},{"family":"Ye","given":"Qinghao"},{"family":"Yan","given":"Ming"},{"family":"Dan","given":"Yuhao"},{"family":"Zhao","given":"Chenlin"},{"family":"Xu","given":"Guohai"},{"family":"Li","given":"Chenliang"},{"family":"Tian","given":"Junfeng"},{"family":"Qi","given":"Qian"},{"family":"Zhang","given":"Ji"},{"family":"Huang","given":"Fei"}],"citation-key":"mPLUG-DocOwlYe2307.02499a","DOI":"10.48550/arXiv.2307.02499","issued":{"date-parts":[["2023",7,4]]},"number":"arXiv:2307.02499","publisher":"arXiv","source":"arXiv.org","title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding","title-short":"mPLUG-DocOwl","type":"article","URL":"http://arxiv.org/abs/2307.02499"},
  {"id":"NougatBlecher2308.13418","abstract":"Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Blecher","given":"Lukas"},{"family":"Cucurull","given":"Guillem"},{"family":"Scialom","given":"Thomas"},{"family":"Stojnic","given":"Robert"}],"citation-key":"NougatBlecher2308.13418","DOI":"10.48550/arXiv.2308.13418","issued":{"date-parts":[["2023",8,25]]},"number":"arXiv:2308.13418","publisher":"arXiv","source":"arXiv.org","title":"Nougat: Neural Optical Understanding for Academic Documents","title-short":"Nougat","type":"article","URL":"http://arxiv.org/abs/2308.13418"},
  {"id":"OCRfreeDocumentUnderstandingKim2111.15664","abstract":"Understanding document images (e.g., invoices) is a core but challenging task since it requires complex functions such as reading text and a holistic understanding of the document. Current Visual Document Understanding (VDU) methods outsource the task of reading text to off-the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs. Although such OCR-based approaches have shown promising performance, they suffer from 1) high computational costs for using OCR; 2) inflexibility of OCR models on languages or types of document; 3) OCR error propagation to the subsequent process. To address these issues, in this paper, we introduce a novel OCR-free VDU model named Donut, which stands for Document understanding transformer. As the first step in OCR-free VDU research, we propose a simple architecture (i.e., Transformer) with a pre-training objective (i.e., cross-entropy loss). Donut is conceptually simple yet effective. Through extensive experiments and analyses, we show a simple OCR-free VDU model, Donut, achieves state-of-the-art performances on various VDU tasks in terms of both speed and accuracy. In addition, we offer a synthetic data generator that helps the model pre-training to be flexible in various languages and domains. The code, trained model and synthetic data are available at https://github.com/clovaai/donut.","accessed":{"date-parts":[["2024",3,21]]},"author":[{"family":"Kim","given":"Geewook"},{"family":"Hong","given":"Teakgyu"},{"family":"Yim","given":"Moonbin"},{"family":"Nam","given":"Jeongyeon"},{"family":"Park","given":"Jinyoung"},{"family":"Yim","given":"Jinyeong"},{"family":"Hwang","given":"Wonseok"},{"family":"Yun","given":"Sangdoo"},{"family":"Han","given":"Dongyoon"},{"family":"Park","given":"Seunghyun"}],"citation-key":"OCRfreeDocumentUnderstandingKim2111.15664","DOI":"10.48550/arXiv.2111.15664","issued":{"date-parts":[["2022",10,6]]},"number":"arXiv:2111.15664","publisher":"arXiv","source":"arXiv.org","title":"OCR-free Document Understanding Transformer","type":"article","URL":"http://arxiv.org/abs/2111.15664"},
  {"id":"OmniParserWan2403.19128","abstract":"Recently, visually-situated text parsing (VsTP) has experienced notable advancements, driven by the increasing demand for automated document understanding and the emergence of Generative Large Language Models (LLMs) capable of processing document-based questions. Various methods have been proposed to address the challenging problem of VsTP. However, due to the diversified targets and heterogeneous schemas, previous works usually design task-specific architectures and objectives for individual tasks, which inadvertently leads to modal isolation and complex workflow. In this paper, we propose a unified paradigm for parsing visually-situated text across diverse scenarios. Specifically, we devise a universal model, called OmniParser, which can simultaneously handle three typical visually-situated text parsing tasks: text spotting, key information extraction, and table recognition. In OmniParser, all tasks share the unified encoder-decoder architecture, the unified objective: point-conditioned text generation, and the unified input & output representation: prompt & structured sequences. Extensive experiments demonstrate that the proposed OmniParser achieves state-of-the-art (SOTA) or highly competitive performances on 7 datasets for the three visually-situated text parsing tasks, despite its unified, concise design. The code is available at https://github.com/AlibabaResearch/AdvancedLiterateMachinery.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Wan","given":"Jianqiang"},{"family":"Song","given":"Sibo"},{"family":"Yu","given":"Wenwen"},{"family":"Liu","given":"Yuliang"},{"family":"Cheng","given":"Wenqing"},{"family":"Huang","given":"Fei"},{"family":"Bai","given":"Xiang"},{"family":"Yao","given":"Cong"},{"family":"Yang","given":"Zhibo"}],"citation-key":"OmniParserWan2403.19128","DOI":"10.48550/arXiv.2403.19128","issued":{"date-parts":[["2024",3,27]]},"number":"arXiv:2403.19128","publisher":"arXiv","source":"arXiv.org","title":"OmniParser: A Unified Framework for Text Spotting, Key Information Extraction and Table Recognition","title-short":"OmniParser","type":"article","URL":"http://arxiv.org/abs/2403.19128"},
  {"id":"PEaCEZhang2403.15724","abstract":"Optical Character Recognition (OCR) is an established task with the objective of identifying the text present in an image. While many off-the-shelf OCR models exist, they are often trained for either scientific (e.g., formulae) or generic printed English text. Extracting text from chemistry publications requires an OCR model that is capable in both realms. Nougat, a recent tool, exhibits strong ability to parse academic documents, but is unable to parse tables in PubMed articles, which comprises a significant part of the academic community and is the focus of this work. To mitigate this gap, we present the Printed English and Chemical Equations (PEaCE) dataset, containing both synthetic and real-world records, and evaluate the efficacy of transformer-based OCR models when trained on this resource. Given that real-world records contain artifacts not present in synthetic records, we propose transformations that mimic such qualities. We perform a suite of experiments to explore the impact of patch size, multi-domain training, and our proposed transformations, ultimately finding that models with a small patch size trained on multiple domains using the proposed transformations yield the best performance. Our dataset and code is available at https://github.com/ZN1010/PEaCE.","accessed":{"date-parts":[["2024",4,10]]},"author":[{"family":"Zhang","given":"Nan"},{"family":"Heaton","given":"Connor"},{"family":"Okonsky","given":"Sean Timothy"},{"family":"Mitra","given":"Prasenjit"},{"family":"Toraman","given":"Hilal Ezgi"}],"citation-key":"PEaCEZhang2403.15724","DOI":"10.48550/arXiv.2403.15724","issued":{"date-parts":[["2024",3,23]]},"number":"arXiv:2403.15724","publisher":"arXiv","source":"arXiv.org","title":"PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on Scientific Documents","title-short":"PEaCE","type":"article","URL":"http://arxiv.org/abs/2403.15724"},
  {"id":"Pix2StructLee2210.03347","abstract":"Visually-situated language is ubiquitous -- sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.","accessed":{"date-parts":[["2024",7,15]]},"author":[{"family":"Lee","given":"Kenton"},{"family":"Joshi","given":"Mandar"},{"family":"Turc","given":"Iulia"},{"family":"Hu","given":"Hexiang"},{"family":"Liu","given":"Fangyu"},{"family":"Eisenschlos","given":"Julian"},{"family":"Khandelwal","given":"Urvashi"},{"family":"Shaw","given":"Peter"},{"family":"Chang","given":"Ming-Wei"},{"family":"Toutanova","given":"Kristina"}],"citation-key":"Pix2StructLee2210.03347","DOI":"10.48550/arXiv.2210.03347","issued":{"date-parts":[["2023",6,15]]},"number":"arXiv:2210.03347","publisher":"arXiv","source":"arXiv.org","title":"Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding","title-short":"Pix2Struct","type":"article","URL":"http://arxiv.org/abs/2210.03347"},
  {"id":"Qwen-VLBai2308.12966b","abstract":"In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Bai","given":"Jinze"},{"family":"Bai","given":"Shuai"},{"family":"Yang","given":"Shusheng"},{"family":"Wang","given":"Shijie"},{"family":"Tan","given":"Sinan"},{"family":"Wang","given":"Peng"},{"family":"Lin","given":"Junyang"},{"family":"Zhou","given":"Chang"},{"family":"Zhou","given":"Jingren"}],"citation-key":"Qwen-VLBai2308.12966b","DOI":"10.48550/arXiv.2308.12966","issued":{"date-parts":[["2023",10,12]]},"number":"arXiv:2308.12966","publisher":"arXiv","source":"arXiv.org","title":"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond","title-short":"Qwen-VL","type":"article","URL":"http://arxiv.org/abs/2308.12966"},
  {"id":"SCOBKim2309.12382","abstract":"Inspired by the great success of language model (LM)-based pre-training, recent studies in visual document understanding have explored LM-based pre-training methods for modeling text within document images. Among them, pre-training that reads all text from an image has shown promise, but often exhibits instability and even fails when applied to broader domains, such as those involving both visual documents and scene text images. This is a substantial limitation for real-world scenarios, where the processing of text image inputs in diverse domains is essential. In this paper, we investigate effective pre-training tasks in the broader domains and also propose a novel pre-training method called SCOB that leverages character-wise supervised contrastive learning with online text rendering to effectively pre-train document and scene text domains by bridging the domain gap. Moreover, SCOB enables weakly supervised learning, significantly reducing annotation costs. Extensive benchmarks demonstrate that SCOB generally improves vanilla pre-training methods and achieves comparable performance to state-of-the-art methods. Our findings suggest that SCOB can be served generally and effectively for read-type pre-training methods. The code will be available at https://github.com/naver-ai/scob.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Kim","given":"Daehee"},{"family":"Kim","given":"Yoonsik"},{"family":"Kim","given":"DongHyun"},{"family":"Lim","given":"Yumin"},{"family":"Kim","given":"Geewook"},{"family":"Kil","given":"Taeho"}],"citation-key":"SCOBKim2309.12382","DOI":"10.48550/arXiv.2309.12382","issued":{"date-parts":[["2023",9,21]]},"number":"arXiv:2309.12382","publisher":"arXiv","source":"arXiv.org","title":"SCOB: Universal Text Understanding via Character-wise Supervised Contrastive Learning with Online Text Rendering for Bridging Domain Gap","title-short":"SCOB","type":"article","URL":"http://arxiv.org/abs/2309.12382"},
  {"id":"SelfDocLi2106.03331","abstract":"We propose SelfDoc, a task-agnostic pre-training framework for document image understanding. Because documents are multimodal and are intended for sequential reading, our framework exploits the positional, textual, and visual information of every semantically meaningful component in a document, and it models the contextualization between each block of content. Unlike existing document pre-training models, our model is coarse-grained instead of treating individual words as input, therefore avoiding an overly fine-grained with excessive contextualization. Beyond that, we introduce cross-modal learning in the model pre-training phase to fully leverage multimodal information from unlabeled documents. For downstream usage, we propose a novel modality-adaptive attention mechanism for multimodal feature fusion by adaptively emphasizing language and vision signals. Our framework benefits from self-supervised pre-training on documents without requiring annotations by a feature masking training strategy. It achieves superior performance on multiple downstream tasks with significantly fewer document images used in the pre-training stage compared to previous works.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Li","given":"Peizhao"},{"family":"Gu","given":"Jiuxiang"},{"family":"Kuen","given":"Jason"},{"family":"Morariu","given":"Vlad I."},{"family":"Zhao","given":"Handong"},{"family":"Jain","given":"Rajiv"},{"family":"Manjunatha","given":"Varun"},{"family":"Liu","given":"Hongfu"}],"citation-key":"SelfDocLi2106.03331","DOI":"10.48550/arXiv.2106.03331","issued":{"date-parts":[["2021",6,7]]},"number":"arXiv:2106.03331","publisher":"arXiv","source":"arXiv.org","title":"SelfDoc: Self-Supervised Document Representation Learning","title-short":"SelfDoc","type":"article","URL":"http://arxiv.org/abs/2106.03331"},
  {"id":"StrucTexTLi2021","abstract":"Structured text understanding on Visually Rich Documents (VRDs) is a crucial part of Document Intelligence. Due to the complexity of content and layout in VRDs, structured text understanding has been a challenging task. Most existing studies decoupled this problem into two sub-tasks: entity labeling and entity linking, which require an entire understanding of the context of documents at both token and segment levels. However, little work has been concerned with the solutions that efficiently extract the structured data from different levels. This paper proposes a unified framework named StrucTexT, which is flexible and effective for handling both sub-tasks. Specifically, based on the transformer, we introduce a segment-token aligned encoder to deal with the entity labeling and entity linking tasks at different levels of granularity. Moreover, we design a novel pre-training strategy with three self-supervised tasks to learn a richer representation. StrucTexT uses the existing Masked Visual Language Modeling task and the new Sentence Length Prediction and Paired Boxes Direction tasks to incorporate the multi-modal information across text, image, and layout. We evaluate our method for structured text understanding at segment-level and token-level and show it outperforms the state-of-the-art counterparts with significantly superior performance on the FUNSD, SROIE, and EPHOIE datasets.","accessed":{"date-parts":[["2024",7,11]]},"author":[{"family":"Li","given":"Yulin"},{"family":"Qian","given":"Yuxi"},{"family":"Yu","given":"Yuechen"},{"family":"Qin","given":"Xiameng"},{"family":"Zhang","given":"Chengquan"},{"family":"Liu","given":"Yan"},{"family":"Yao","given":"Kun"},{"family":"Han","given":"Junyu"},{"family":"Liu","given":"Jingtuo"},{"family":"Ding","given":"Errui"}],"citation-key":"StrucTexTLi2021","collection-title":"MM '21","container-title":"Proceedings of the 29th ACM International Conference on Multimedia","DOI":"10.1145/3474085.3475345","event-place":"New York, NY, USA","ISBN":"978-1-4503-8651-7","issued":{"date-parts":[["2021",10,17]]},"page":"1912–1920","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"StrucTexT: Structured Text Understanding with Multi-Modal Transformers","title-short":"StrucTexT","type":"paper-conference","URL":"https://doi.org/10.1145/3474085.3475345"},
  {"id":"StrucTexTLi2108.02923","abstract":"Structured text understanding on Visually Rich Documents (VRDs) is a crucial part of Document Intelligence. Due to the complexity of content and layout in VRDs, structured text understanding has been a challenging task. Most existing studies decoupled this problem into two sub-tasks: entity labeling and entity linking, which require an entire understanding of the context of documents at both token and segment levels. However, little work has been concerned with the solutions that efficiently extract the structured data from different levels. This paper proposes a unified framework named StrucTexT, which is flexible and effective for handling both sub-tasks. Specifically, based on the transformer, we introduce a segment-token aligned encoder to deal with the entity labeling and entity linking tasks at different levels of granularity. Moreover, we design a novel pre-training strategy with three self-supervised tasks to learn a richer representation. StrucTexT uses the existing Masked Visual Language Modeling task and the new Sentence Length Prediction and Paired Boxes Direction tasks to incorporate the multi-modal information across text, image, and layout. We evaluate our method for structured text understanding at segment-level and token-level and show it outperforms the state-of-the-art counterparts with significantly superior performance on the FUNSD, SROIE, and EPHOIE datasets.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Li","given":"Yulin"},{"family":"Qian","given":"Yuxi"},{"family":"Yu","given":"Yuchen"},{"family":"Qin","given":"Xiameng"},{"family":"Zhang","given":"Chengquan"},{"family":"Liu","given":"Yan"},{"family":"Yao","given":"Kun"},{"family":"Han","given":"Junyu"},{"family":"Liu","given":"Jingtuo"},{"family":"Ding","given":"Errui"}],"citation-key":"StrucTexTLi2108.02923","DOI":"10.48550/arXiv.2108.02923","issued":{"date-parts":[["2021",11,8]]},"number":"arXiv:2108.02923","publisher":"arXiv","source":"arXiv.org","title":"StrucTexT: Structured Text Understanding with Multi-Modal Transformers","title-short":"StrucTexT","type":"article","URL":"http://arxiv.org/abs/2108.02923"},
  {"id":"StrucTexTv2Yu2303.00289","abstract":"In this paper, we present StrucTexTv2, an effective document image pre-training framework, by performing masked visual-textual prediction. It consists of two self-supervised pre-training tasks: masked image modeling and masked language modeling, based on text region-level image masking. The proposed method randomly masks some image regions according to the bounding box coordinates of text words. The objectives of our pre-training tasks are reconstructing the pixels of masked image regions and the corresponding masked tokens simultaneously. Hence the pre-trained encoder can capture more textual semantics in comparison to the masked image modeling that usually predicts the masked image patches. Compared to the masked multi-modal modeling methods for document image understanding that rely on both the image and text modalities, StrucTexTv2 models image-only input and potentially deals with more application scenarios free from OCR pre-processing. Extensive experiments on mainstream benchmarks of document image understanding demonstrate the effectiveness of StrucTexTv2. It achieves competitive or even new state-of-the-art performance in various downstream tasks such as image classification, layout analysis, table structure recognition, document OCR, and information extraction under the end-to-end scenario.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Yu","given":"Yuechen"},{"family":"Li","given":"Yulin"},{"family":"Zhang","given":"Chengquan"},{"family":"Zhang","given":"Xiaoqiang"},{"family":"Guo","given":"Zengyuan"},{"family":"Qin","given":"Xiameng"},{"family":"Yao","given":"Kun"},{"family":"Han","given":"Junyu"},{"family":"Ding","given":"Errui"},{"family":"Wang","given":"Jingdong"}],"citation-key":"StrucTexTv2Yu2303.00289","DOI":"10.48550/arXiv.2303.00289","issued":{"date-parts":[["2023",3,1]]},"number":"arXiv:2303.00289","publisher":"arXiv","source":"arXiv.org","title":"StrucTexTv2: Masked Visual-Textual Prediction for Document Image Pre-training","title-short":"StrucTexTv2","type":"article","URL":"http://arxiv.org/abs/2303.00289"},
  {"id":"StrucTexTv3Lyu2405.21013","abstract":"Text-rich images have significant and extensive value, deeply integrated into various aspects of human life. Notably, both visual cues and linguistic symbols in text-rich images play crucial roles in information transmission but are accompanied by diverse challenges. Therefore, the efficient and effective understanding of text-rich images is a crucial litmus test for the capability of Vision-Language Models. We have crafted an efficient vision-language model, StrucTexTv3, tailored to tackle various intelligent tasks for text-rich images. The significant design of StrucTexTv3 is presented in the following aspects: Firstly, we adopt a combination of an effective multi-scale reduced visual transformer and a multi-granularity token sampler (MG-Sampler) as a visual token generator, successfully solving the challenges of high-resolution input and complex representation learning for text-rich images. Secondly, we enhance the perception and comprehension abilities of StrucTexTv3 through instruction learning, seamlessly integrating various text-oriented tasks into a unified framework. Thirdly, we have curated a comprehensive collection of high-quality text-rich images, abbreviated as TIM-30M, encompassing diverse scenarios like incidental scenes, office documents, web pages, and screenshots, thereby improving the robustness of our model. Our method achieved SOTA results in text-rich image perception tasks, and significantly improved performance in comprehension tasks. Among multimodal models with LLM decoder of approximately 1.8B parameters, it stands out as a leader, which also makes the deployment of edge devices feasible. In summary, the StrucTexTv3 model, featuring efficient structural design, outstanding performance, and broad adaptability, offers robust support for diverse intelligent application tasks involving text-rich images, thus exhibiting immense potential for widespread application.","accessed":{"date-parts":[["2024",6,20]]},"author":[{"family":"Lyu","given":"Pengyuan"},{"family":"Li","given":"Yulin"},{"family":"Zhou","given":"Hao"},{"family":"Ma","given":"Weihong"},{"family":"Wan","given":"Xingyu"},{"family":"Xie","given":"Qunyi"},{"family":"Wu","given":"Liang"},{"family":"Zhang","given":"Chengquan"},{"family":"Yao","given":"Kun"},{"family":"Ding","given":"Errui"},{"family":"Wang","given":"Jingdong"}],"citation-key":"StrucTexTv3Lyu2405.21013","DOI":"10.48550/arXiv.2405.21013","issued":{"date-parts":[["2024",6,4]]},"number":"arXiv:2405.21013","publisher":"arXiv","source":"arXiv.org","title":"StrucTexTv3: An Efficient Vision-Language Model for Text-rich Image Perception, Comprehension, and Beyond","title-short":"StrucTexTv3","type":"article","URL":"http://arxiv.org/abs/2405.21013"},
  {"id":"TextCoTLuan2404.09797","abstract":"The advent of Large Multimodal Models (LMMs) has sparked a surge in research aimed at harnessing their remarkable reasoning abilities. However, for understanding text-rich images, challenges persist in fully leveraging the potential of LMMs, and existing methods struggle with effectively processing high-resolution images. In this work, we propose TextCoT, a novel Chain-of-Thought framework for text-rich image understanding. TextCoT utilizes the captioning ability of LMMs to grasp the global context of the image and the grounding capability to examine local textual regions. This allows for the extraction of both global and local visual information, facilitating more accurate question-answering. Technically, TextCoT consists of three stages, including image overview, coarse localization, and fine-grained observation. The image overview stage provides a comprehensive understanding of the global scene information, and the coarse localization stage approximates the image area containing the answer based on the question asked. Then, integrating the obtained global image descriptions, the final stage further examines specific regions to provide accurate answers. Our method is free of extra training, offering immediate plug-and-play functionality. Extensive experiments are conducted on a series of text-rich image question-answering benchmark datasets based on several advanced LMMs, and the results demonstrate the effectiveness and strong generalization ability of our method. Code is available at https://github.com/bzluan/TextCoT.","accessed":{"date-parts":[["2024",6,20]]},"author":[{"family":"Luan","given":"Bozhi"},{"family":"Feng","given":"Hao"},{"family":"Chen","given":"Hong"},{"family":"Wang","given":"Yonghui"},{"family":"Zhou","given":"Wengang"},{"family":"Li","given":"Houqiang"}],"citation-key":"TextCoTLuan2404.09797","DOI":"10.48550/arXiv.2404.09797","issued":{"date-parts":[["2024",4,15]]},"number":"arXiv:2404.09797","publisher":"arXiv","source":"arXiv.org","title":"TextCoT: Zoom In for Enhanced Multimodal Text-Rich Image Understanding","title-short":"TextCoT","type":"article","URL":"http://arxiv.org/abs/2404.09797"},
  {"id":"TextMonkeyLiu2403.04473","abstract":"We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention with zero-initialization, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model's performance. Moreover, by expanding our model's capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. It also learns to perform screenshot tasks through finetuning. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9\\% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code will be released at https://github.com/Yuliang-Liu/Monkey.","accessed":{"date-parts":[["2024",6,6]]},"author":[{"family":"Liu","given":"Yuliang"},{"family":"Yang","given":"Biao"},{"family":"Liu","given":"Qiang"},{"family":"Li","given":"Zhang"},{"family":"Ma","given":"Zhiyin"},{"family":"Zhang","given":"Shuo"},{"family":"Bai","given":"Xiang"}],"citation-key":"TextMonkeyLiu2403.04473","DOI":"10.48550/arXiv.2403.04473","issued":{"date-parts":[["2024",3,15]]},"number":"arXiv:2403.04473","publisher":"arXiv","source":"arXiv.org","title":"TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document","title-short":"TextMonkey","type":"article","URL":"http://arxiv.org/abs/2403.04473"},
  {"id":"TextSquareTang2404.12803","abstract":"Text-centric visual question answering (VQA) has made great strides with the development of Multimodal Large Language Models (MLLMs), yet open-source models still fall short of leading models like GPT4V and Gemini, partly due to a lack of extensive, high-quality instruction tuning data. To this end, we introduce a new approach for creating a massive, high-quality instruction-tuning dataset, Square-10M, which is generated using closed-source MLLMs. The data construction process, termed Square, consists of four steps: Self-Questioning, Answering, Reasoning, and Evaluation. Our experiments with Square-10M led to three key findings: 1) Our model, TextSquare, considerably surpasses open-source previous state-of-the-art Text-centric MLLMs and sets a new standard on OCRBench(62.2%). It even outperforms top-tier models like GPT4V and Gemini in 6 of 10 text-centric benchmarks. 2) Additionally, we demonstrate the critical role of VQA reasoning data in offering comprehensive contextual insights for specific questions. This not only improves accuracy but also significantly mitigates hallucinations. Specifically, TextSquare scores an average of 75.1% across four general VQA and hallucination evaluation datasets, outperforming previous state-of-the-art models. 3) Notably, the phenomenon observed in scaling text-centric VQA datasets reveals a vivid pattern: the exponential increase of instruction tuning data volume is directly proportional to the improvement in model performance, thereby validating the necessity of the dataset scale and the high quality of Square-10M.","accessed":{"date-parts":[["2024",6,20]]},"author":[{"family":"Tang","given":"Jingqun"},{"family":"Lin","given":"Chunhui"},{"family":"Zhao","given":"Zhen"},{"family":"Wei","given":"Shu"},{"family":"Wu","given":"Binghong"},{"family":"Liu","given":"Qi"},{"family":"Feng","given":"Hao"},{"family":"Li","given":"Yang"},{"family":"Wang","given":"Siqi"},{"family":"Liao","given":"Lei"},{"family":"Shi","given":"Wei"},{"family":"Liu","given":"Yuliang"},{"family":"Liu","given":"Hao"},{"family":"Xie","given":"Yuan"},{"family":"Bai","given":"Xiang"},{"family":"Huang","given":"Can"}],"citation-key":"TextSquareTang2404.12803","DOI":"10.48550/arXiv.2404.12803","issued":{"date-parts":[["2024",4,19]]},"number":"arXiv:2404.12803","publisher":"arXiv","source":"arXiv.org","title":"TextSquare: Scaling up Text-Centric Visual Instruction Tuning","title-short":"TextSquare","type":"article","URL":"http://arxiv.org/abs/2404.12803"},
  {"id":"TowardsImprovingDocumentUnderstandingWang2311.13194","abstract":"In the field of document understanding, significant advances have been made in the fine-tuning of Multimodal Large Language Models (MLLMs) with instruction-following data. Nevertheless, the potential of text-grounding capability within text-rich scenarios remains underexplored. In this paper, we present a text-grounding document understanding model, termed TGDoc, which addresses this deficiency by enhancing MLLMs with the ability to discern the spatial positioning of text within images. Empirical evidence suggests that text-grounding improves the model's interpretation of textual content, thereby elevating its proficiency in comprehending text-rich images. Specifically, we compile a dataset containing 99K PowerPoint presentations sourced from the internet. We formulate instruction tuning tasks including text detection, recognition, and spotting to facilitate the cohesive alignment between the visual encoder and large language model. Moreover, we curate a collection of text-rich images and prompt the text-only GPT-4 to generate 12K high-quality conversations, featuring textual locations within text-rich scenarios. By integrating text location data into the instructions, TGDoc is adept at discerning text locations during the visual question process. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple text-rich benchmarks, validating the effectiveness of our method.","accessed":{"date-parts":[["2024",7,15]]},"author":[{"family":"Wang","given":"Yonghui"},{"family":"Zhou","given":"Wengang"},{"family":"Feng","given":"Hao"},{"family":"Zhou","given":"Keyi"},{"family":"Li","given":"Houqiang"}],"citation-key":"TowardsImprovingDocumentUnderstandingWang2311.13194","DOI":"10.48550/arXiv.2311.13194","issued":{"date-parts":[["2023",12,15]]},"number":"arXiv:2311.13194","publisher":"arXiv","source":"arXiv.org","title":"Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs","title-short":"Towards Improving Document Understanding","type":"article","URL":"http://arxiv.org/abs/2311.13194"},
  {"id":"UniDocFeng2308.11592","abstract":"In the era of Large Language Models (LLMs), tremendous strides have been made in the field of multimodal understanding. However, existing advanced algorithms are limited to effectively utilizing the immense representation capabilities and rich world knowledge inherent to these large pre-trained models, and the beneficial connections among tasks within the context of text-rich scenarios have not been sufficiently explored. In this work, we introduce UniDoc, a novel multimodal model equipped with text detection and recognition capabilities, which are deficient in existing approaches. Moreover, UniDoc capitalizes on the beneficial interactions among tasks to enhance the performance of each individual task. To implement UniDoc, we perform unified multimodal instruct tuning on the contributed large-scale instruction following datasets. Quantitative and qualitative experimental results show that UniDoc sets state-of-the-art scores across multiple challenging benchmarks. To the best of our knowledge, this is the first large multimodal model capable of simultaneous text detection, recognition, spotting, and understanding.","accessed":{"date-parts":[["2024",7,14]]},"author":[{"family":"Feng","given":"Hao"},{"family":"Wang","given":"Zijian"},{"family":"Tang","given":"Jingqun"},{"family":"Lu","given":"Jinghui"},{"family":"Zhou","given":"Wengang"},{"family":"Li","given":"Houqiang"},{"family":"Huang","given":"Can"}],"citation-key":"UniDocFeng2308.11592","DOI":"10.48550/arXiv.2308.11592","issued":{"date-parts":[["2023",9,2]]},"number":"arXiv:2308.11592","publisher":"arXiv","source":"arXiv.org","title":"UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding","title-short":"UniDoc","type":"article","URL":"http://arxiv.org/abs/2308.11592"},
  {"id":"UnifyingVisionTextTang2212.02623","abstract":"We propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and layout modalities via masked image reconstruction. To the best of our knowledge, this is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization. Our method sets the state-of-the-art on 8 Document AI tasks, e.g., document understanding and QA, across diverse data domains like finance reports, academic papers, and websites. UDOP ranks first on the leaderboard of the Document Understanding Benchmark.","accessed":{"date-parts":[["2024",3,21]]},"author":[{"family":"Tang","given":"Zineng"},{"family":"Yang","given":"Ziyi"},{"family":"Wang","given":"Guoxin"},{"family":"Fang","given":"Yuwei"},{"family":"Liu","given":"Yang"},{"family":"Zhu","given":"Chenguang"},{"family":"Zeng","given":"Michael"},{"family":"Zhang","given":"Cha"},{"family":"Bansal","given":"Mohit"}],"citation-key":"UnifyingVisionTextTang2212.02623","issued":{"date-parts":[["2023",3,13]]},"number":"arXiv:2212.02623","publisher":"arXiv","source":"arXiv.org","title":"Unifying Vision, Text, and Layout for Universal Document Processing","type":"article","URL":"http://arxiv.org/abs/2212.02623","version":"3"},
  {"id":"UReaderYe2023","abstract":"Text is ubiquitous in our visual world, conveying crucial information, such as in documents, websites, and everyday photographs. In this work, we propose UReader, a first exploration of universal OCR-free visually-situated language understanding based on the Multimodal Large Language Model (MLLM). By leveraging the shallow text recognition ability of the MLLM, we only finetuned 1.2% parameters and the training cost is much lower than previous work following domain-specific pretraining and finetuning paradigms. Concretely, UReader is jointly finetuned on a wide range of Visually-situated Language Understanding tasks via a unified instruction format. To enhance the visual text and semantic understanding, we further apply two auxiliary tasks with the same format, namely text reading and key points generation tasks. We design a shape-adaptive cropping module before the encoder-decoder architecture of MLLM to leverage the frozen low-resolution vision encoder for processing high-resolution images. Without downstream finetuning, our single model achieves state-of-the-art ocr-free performance in 8 out of 10 visually-situated language understanding tasks, across 5 domains: documents, tables, charts, natural images, and webpage screenshots. Codes and instruction-tuning datasets will be released.","accessed":{"date-parts":[["2024",6,6]]},"author":[{"family":"Ye","given":"Jiabo"},{"family":"Hu","given":"Anwen"},{"family":"Xu","given":"Haiyang"},{"family":"Ye","given":"Qinghao"},{"family":"Yan","given":"Ming"},{"family":"Xu","given":"Guohai"},{"family":"Li","given":"Chenliang"},{"family":"Tian","given":"Junfeng"},{"family":"Qian","given":"Qi"},{"family":"Zhang","given":"Ji"},{"family":"Jin","given":"Qin"},{"family":"He","given":"Liang"},{"family":"Lin","given":"Xin Alex"},{"family":"Huang","given":"Fei"}],"citation-key":"UReaderYe2023","container-title":"arXiv.org","issued":{"date-parts":[["2023",10,8]]},"language":"en","title":"UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model","title-short":"UReader","type":"webpage","URL":"https://arxiv.org/abs/2310.05126v1"},
  {"id":"VisionGridTransformerDa2308.14978","abstract":"Document pre-trained models and grid-based models have proven to be very effective on various tasks in Document AI. However, for the document layout analysis (DLA) task, existing document pre-trained models, even those pre-trained in a multi-modal fashion, usually rely on either textual features or visual features. Grid-based models for DLA are multi-modality but largely neglect the effect of pre-training. To fully leverage multi-modal information and exploit pre-training techniques to learn better representation for DLA, in this paper, we present VGT, a two-stream Vision Grid Transformer, in which Grid Transformer (GiT) is proposed and pre-trained for 2D token-level and segment-level semantic understanding. Furthermore, a new dataset named D$^4$LA, which is so far the most diverse and detailed manually-annotated benchmark for document layout analysis, is curated and released. Experiment results have illustrated that the proposed VGT model achieves new state-of-the-art results on DLA tasks, e.g. PubLayNet ($95.7\\%$$\\rightarrow$$96.2\\%$), DocBank ($79.6\\%$$\\rightarrow$$84.1\\%$), and D$^4$LA ($67.7\\%$$\\rightarrow$$68.8\\%$). The code and models as well as the D$^4$LA dataset will be made publicly available ~\\url{https://github.com/AlibabaResearch/AdvancedLiterateMachinery}.","accessed":{"date-parts":[["2024",7,12]]},"author":[{"family":"Da","given":"Cheng"},{"family":"Luo","given":"Chuwei"},{"family":"Zheng","given":"Qi"},{"family":"Yao","given":"Cong"}],"citation-key":"VisionGridTransformerDa2308.14978","DOI":"10.48550/arXiv.2308.14978","issued":{"date-parts":[["2023",8,28]]},"number":"arXiv:2308.14978","publisher":"arXiv","source":"arXiv.org","title":"Vision Grid Transformer for Document Layout Analysis","type":"article","URL":"http://arxiv.org/abs/2308.14978"},
  {"id":"VRDUWang2023","abstract":"Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry. Although recent multi-modal language models have achieved impressive results, we find that existing benchmarks do not reflect the complexity of real documents seen in industry. In this work, we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding (VRDU). VRDU contains two datasets that represent several challenges: rich schema including diverse data types as well as hierarchical entities, complex templates including tables and multi-column layouts, and diversity of different layouts (templates) within a single document type. We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results. We report the performance of strong baselines and offer three observations: (1) generalizing to new document templates is still very challenging, (2) few-shot performance has a lot of headroom, and (3) models struggle with hierarchical fields such as line-items in an invoice. We plan to open source the benchmark and the evaluation toolkit. We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents.","accessed":{"date-parts":[["2024",3,19]]},"author":[{"family":"Wang","given":"Zilong"},{"family":"Zhou","given":"Yichao"},{"family":"Wei","given":"Wei"},{"family":"Lee","given":"Chen-Yu"},{"family":"Tata","given":"Sandeep"}],"citation-key":"VRDUWang2023","container-title":"Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining","DOI":"10.1145/3580305.3599929","issued":{"date-parts":[["2023",8,6]]},"page":"5184-5193","source":"arXiv.org","title":"VRDU: A Benchmark for Visually-rich Document Understanding","title-short":"VRDU","type":"paper-conference","URL":"http://arxiv.org/abs/2211.15421"},
  {"id":"XYLayoutLMGu2203.06947","abstract":"Recently, various multimodal networks for Visually-Rich Document Understanding(VRDU) have been proposed, showing the promotion of transformers by integrating visual and layout information with the text embeddings. However, most existing approaches utilize the position embeddings to incorporate the sequence information, neglecting the noisy improper reading order obtained by OCR tools. In this paper, we propose a robust layout-aware multimodal network named XYLayoutLM to capture and leverage rich layout information from proper reading orders produced by our Augmented XY Cut. Moreover, a Dilated Conditional Position Encoding module is proposed to deal with the input sequence of variable lengths, and it additionally extracts local layout information from both textual and visual modalities while generating position embeddings. Experiment results show that our XYLayoutLM achieves competitive results on document understanding tasks.","accessed":{"date-parts":[["2024",7,13]]},"author":[{"family":"Gu","given":"Zhangxuan"},{"family":"Meng","given":"Changhua"},{"family":"Wang","given":"Ke"},{"family":"Lan","given":"Jun"},{"family":"Wang","given":"Weiqiang"},{"family":"Gu","given":"Ming"},{"family":"Zhang","given":"Liqing"}],"citation-key":"XYLayoutLMGu2203.06947","DOI":"10.48550/arXiv.2203.06947","issued":{"date-parts":[["2022",3,15]]},"number":"arXiv:2203.06947","publisher":"arXiv","source":"arXiv.org","title":"XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding","title-short":"XYLayoutLM","type":"article","URL":"http://arxiv.org/abs/2203.06947"},
  {"id":"DocLLMWang2401.00908","abstract":"Enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.","accessed":{"date-parts":[["2024",3,14]]},"author":[{"family":"Wang","given":"Dongsheng"},{"family":"Raman","given":"Natraj"},{"family":"Sibue","given":"Mathieu"},{"family":"Ma","given":"Zhiqiang"},{"family":"Babkin","given":"Petr"},{"family":"Kaur","given":"Simerjot"},{"family":"Pei","given":"Yulong"},{"family":"Nourbakhsh","given":"Armineh"},{"family":"Liu","given":"Xiaomo"}],"citation-key":"DocLLMWang2401.00908","DOI":"10.48550/arXiv.2401.00908","issued":{"date-parts":[["2023",12,31]]},"number":"arXiv:2401.00908","publisher":"arXiv","source":"arXiv.org","title":"DocLLM: A layout-aware generative language model for multimodal document understanding","title-short":"DocLLM","type":"article","URL":"http://arxiv.org/abs/2401.00908"},
  {"id":"DeepSeek-VLLu2403.05525","abstract":"We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. Our approach is structured around three key dimensions: We strive to ensure our data is diverse, scalable, and extensively covers real-world scenarios including web screenshots, PDFs, OCR, charts, and knowledge-based content, aiming for a comprehensive representation of practical contexts. Further, we create a use case taxonomy from real user scenarios and construct an instruction tuning dataset accordingly. The fine-tuning with this dataset substantially improves the model's user experience in practical applications. Considering efficiency and the demands of most real-world scenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently processes high-resolution images (1024 x 1024), while maintaining a relatively low computational overhead. This design choice ensures the model's ability to capture critical semantic and detailed information across various visual tasks. We posit that a proficient Vision-Language Model should, foremost, possess strong language abilities. To ensure the preservation of LLM capabilities during pretraining, we investigate an effective VL pretraining strategy by integrating LLM training from the beginning and carefully managing the competitive dynamics observed between vision and language modalities. The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user experiences as a vision-language chatbot in real-world applications, achieving state-of-the-art or competitive performance across a wide range of visual-language benchmarks at the same model size while maintaining robust performance on language-centric benchmarks. We have made both 1.3B and 7B models publicly accessible to foster innovations based on this foundation model.","accessed":{"date-parts":[["2024",6,6]]},"author":[{"family":"Lu","given":"Haoyu"},{"family":"Liu","given":"Wen"},{"family":"Zhang","given":"Bo"},{"family":"Wang","given":"Bingxuan"},{"family":"Dong","given":"Kai"},{"family":"Liu","given":"Bo"},{"family":"Sun","given":"Jingxiang"},{"family":"Ren","given":"Tongzheng"},{"family":"Li","given":"Zhuoshu"},{"family":"Yang","given":"Hao"},{"family":"Sun","given":"Yaofeng"},{"family":"Deng","given":"Chengqi"},{"family":"Xu","given":"Hanwei"},{"family":"Xie","given":"Zhenda"},{"family":"Ruan","given":"Chong"}],"citation-key":"DeepSeek-VLLu2403.05525","DOI":"10.48550/arXiv.2403.05525","issued":{"date-parts":[["2024",3,11]]},"number":"arXiv:2403.05525","publisher":"arXiv","source":"arXiv.org","title":"DeepSeek-VL: Towards Real-World Vision-Language Understanding","title-short":"DeepSeek-VL","type":"article","URL":"http://arxiv.org/abs/2403.05525"},
  {"id":"DocXChainYao2310.12430","abstract":"In this report, we introduce DocXChain, a powerful open-source toolchain for document parsing, which is designed and developed to automatically convert the rich information embodied in unstructured documents, such as text, tables and charts, into structured representations that are readable and manipulable by machines. Specifically, basic capabilities, including text detection, text recognition, table structure recognition and layout analysis, are provided. Upon these basic capabilities, we also build a set of fully functional pipelines for document parsing, i.e., general text reading, table parsing, and document structurization, to drive various applications related to documents in real-world scenarios. Moreover, DocXChain is concise, modularized and flexible, such that it can be readily integrated with existing tools, libraries or models (such as LangChain and ChatGPT), to construct more powerful systems that can accomplish more complicated and challenging tasks. The code of DocXChain is publicly available at:~\\url{https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/Applications/DocXChain}","accessed":{"date-parts":[["2024",7,17]]},"author":[{"family":"Yao","given":"Cong"}],"citation-key":"DocXChainYao2310.12430","DOI":"10.48550/arXiv.2310.12430","issued":{"date-parts":[["2023",10,18]]},"number":"arXiv:2310.12430","publisher":"arXiv","source":"arXiv.org","title":"DocXChain: A Powerful Open-Source Toolchain for Document Parsing and Beyond","title-short":"DocXChain","type":"article","URL":"http://arxiv.org/abs/2310.12430"},
  {"id":"EfficientOCRBryan2310.10050","abstract":"Billions of public domain documents remain trapped in hard copy or lack an accurate digitization. Modern natural language processing methods cannot be used to index, retrieve, and summarize their texts; conduct computational textual analyses; or extract information for statistical analyses, and these texts cannot be incorporated into language model training. Given the diversity and sheer quantity of public domain texts, liberating them at scale requires optical character recognition (OCR) that is accurate, extremely cheap to deploy, and sample-efficient to customize to novel collections, languages, and character sets. Existing OCR engines, largely designed for small-scale commercial applications in high resource languages, often fall short of these requirements. EffOCR (EfficientOCR), a novel open-source OCR package, meets both the computational and sample efficiency requirements for liberating texts at scale by abandoning the sequence-to-sequence architecture typically used for OCR, which takes representations from a learned vision model as inputs to a learned language model. Instead, EffOCR models OCR as a character or word-level image retrieval problem. EffOCR is cheap and sample efficient to train, as the model only needs to learn characters' visual appearance and not how they are used in sequence to form language. Models in the EffOCR model zoo can be deployed off-the-shelf with only a few lines of code. Importantly, EffOCR also allows for easy, sample efficient customization with a simple model training interface and minimal labeling requirements due to its sample efficiency. We illustrate the utility of EffOCR by cheaply and accurately digitizing 20 million historical U.S. newspaper scans, evaluating zero-shot performance on randomly selected documents from the U.S. National Archives, and accurately digitizing Japanese documents for which all other OCR solutions failed.","accessed":{"date-parts":[["2024",7,17]]},"author":[{"family":"Bryan","given":"Tom"},{"family":"Carlson","given":"Jacob"},{"family":"Arora","given":"Abhishek"},{"family":"Dell","given":"Melissa"}],"citation-key":"EfficientOCRBryan2310.10050","DOI":"10.48550/arXiv.2310.10050","issued":{"date-parts":[["2023",10,16]]},"number":"arXiv:2310.10050","publisher":"arXiv","source":"arXiv.org","title":"EfficientOCR: An Extensible, Open-Source Package for Efficiently Digitizing World Knowledge","title-short":"EfficientOCR","type":"article","URL":"http://arxiv.org/abs/2310.10050"},
  {"id":"FocusAnywhereFinegrainedLiu2405.14295","abstract":"Modern LVLMs still struggle to achieve fine-grained document understanding, such as OCR/translation/caption for regions of interest to the user, tasks that require the context of the entire page, or even multiple pages. Accordingly, this paper proposes Fox, an effective pipeline, hybrid data, and tuning strategy, that catalyzes LVLMs to focus anywhere on single/multi-page documents. We introduce a novel task to boost the document understanding by making LVLMs focus attention on the document-level region, such as redefining full-page OCR as foreground focus. We employ multiple vision vocabularies to extract visual hybrid knowledge for interleaved document pages (e.g., a page containing a photo). Meanwhile, we render cross-vocabulary vision data as the catalyzer to achieve a full reaction of multiple visual vocabularies and in-document figure understanding. Further, without modifying the weights of multiple vision vocabularies, the above catalyzed fine-grained understanding capabilities can be efficiently tuned to multi-page documents, enabling the model to focus anywhere in both format-free and page-free manners. Besides, we build a benchmark including 9 fine-grained sub-tasks (e.g., region-level OCR/summary, color-guided OCR) to promote document analysis in the community. The experimental results verify the superiority of our model.","accessed":{"date-parts":[["2024",6,6]]},"author":[{"family":"Liu","given":"Chenglong"},{"family":"Wei","given":"Haoran"},{"family":"Chen","given":"Jinyue"},{"family":"Kong","given":"Lingyu"},{"family":"Ge","given":"Zheng"},{"family":"Zhu","given":"Zining"},{"family":"Zhao","given":"Liang"},{"family":"Sun","given":"Jianjian"},{"family":"Han","given":"Chunrui"},{"family":"Zhang","given":"Xiangyu"}],"citation-key":"FocusAnywhereFinegrainedLiu2405.14295","DOI":"10.48550/arXiv.2405.14295","issued":{"date-parts":[["2024",5,23]]},"number":"arXiv:2405.14295","publisher":"arXiv","source":"arXiv.org","title":"Focus Anywhere for Fine-grained Multi-page Document Understanding","type":"article","URL":"http://arxiv.org/abs/2405.14295"},
  {"id":"LaTeXRainbowDuan2023","accessed":{"date-parts":[["2024",6,6]]},"author":[{"family":"Duan","given":"Changxu"},{"family":"Tan","given":"Zhiyin"},{"family":"Bartsch","given":"Sabine"}],"citation-key":"LaTeXRainbowDuan2023","container-title":"Proceedings of the Second Workshop on Information Extraction from Scientific Publications","DOI":"10.18653/v1/2023.wiesp-1.8","editor":[{"family":"Ghosal","given":"Tirthankar"},{"family":"Grezes","given":"Felix"},{"family":"Allen","given":"Thomas"},{"family":"Lockhart","given":"Kelly"},{"family":"Accomazzi","given":"Alberto"},{"family":"Blanco-Cuaresma","given":"Sergi"}],"event-place":"Bali, Indonesia","issued":{"date-parts":[["2023",11]]},"page":"56–67","publisher":"Association for Computational Linguistics","publisher-place":"Bali, Indonesia","source":"ACLWeb","title":"LaTeX Rainbow: Universal LaTeX to PDF Document Semantic & Layout Annotation Framework","title-short":"LaTeX Rainbow","type":"paper-conference","URL":"https://aclanthology.org/2023.wiesp-1.8"},
  {"id":"LOCRSun2403.02127","abstract":"Academic documents are packed with texts, equations, tables, and figures, requiring comprehensive understanding for accurate Optical Character Recognition (OCR). While end-to-end OCR methods offer improved accuracy over layout-based approaches, they often grapple with significant repetition issues, especially with complex layouts in Out-Of-Domain (OOD) documents.To tackle this issue, we propose LOCR, a model that integrates location guiding into the transformer architecture during autoregression. We train the model on a dataset comprising over 77M text-location pairs from 125K academic document pages, including bounding boxes for words, tables and mathematical symbols. LOCR adeptly handles various formatting elements and generates content in Markdown language. It outperforms all existing methods in our test set constructed from arXiv, as measured by edit distance, BLEU, METEOR and F-measure.LOCR also reduces repetition frequency from 4.4% of pages to 0.5% in the arXiv dataset, from 13.2% to 1.3% in OOD quantum physics documents and from 8.1% to 1.8% in OOD marketing documents. Additionally, LOCR features an interactive OCR mode, facilitating the generation of complex documents through a few location prompts from human.","accessed":{"date-parts":[["2024",4,10]]},"author":[{"family":"Sun","given":"Yu"},{"family":"Zhou","given":"Dongzhan"},{"family":"Lin","given":"Chen"},{"family":"He","given":"Conghui"},{"family":"Ouyang","given":"Wanli"},{"family":"Zhong","given":"Han-Sen"}],"citation-key":"LOCRSun2403.02127","DOI":"10.48550/arXiv.2403.02127","issued":{"date-parts":[["2024",3,4]]},"number":"arXiv:2403.02127","publisher":"arXiv","source":"arXiv.org","title":"LOCR: Location-Guided Transformer for Optical Character Recognition","title-short":"LOCR","type":"article","URL":"http://arxiv.org/abs/2403.02127"},
  {"id":"VaryWei2312.06109","abstract":"Modern Large Vision-Language Models (LVLMs) enjoy the same vision vocabulary -- CLIP, which can cover most common vision tasks. However, for some special vision task that needs dense and fine-grained vision perception, e.g., document-level OCR or chart understanding, especially in non-English scenarios, the CLIP-style vocabulary may encounter low efficiency in tokenizing the vision knowledge and even suffer out-of-vocabulary problem. Accordingly, we propose Vary, an efficient and effective method to scale up the vision vocabulary of LVLMs. The procedures of Vary are naturally divided into two folds: the generation and integration of a new vision vocabulary. In the first phase, we devise a vocabulary network along with a tiny decoder-only transformer to produce the desired vocabulary via autoregression. In the next, we scale up the vanilla vision vocabulary by merging the new one with the original one (CLIP), enabling the LVLMs can quickly garner new features. Compared to the popular BLIP-2, MiniGPT4, and LLaVA, Vary can maintain its vanilla capabilities while enjoying more excellent fine-grained perception and understanding ability. Specifically, Vary is competent in new document parsing features (OCR or markdown conversion) while achieving 78.2% ANLS in DocVQA and 36.2% in MMVet. Our code will be publicly available on the homepage.","accessed":{"date-parts":[["2024",6,6]]},"author":[{"family":"Wei","given":"Haoran"},{"family":"Kong","given":"Lingyu"},{"family":"Chen","given":"Jinyue"},{"family":"Zhao","given":"Liang"},{"family":"Ge","given":"Zheng"},{"family":"Yang","given":"Jinrong"},{"family":"Sun","given":"Jianjian"},{"family":"Han","given":"Chunrui"},{"family":"Zhang","given":"Xiangyu"}],"citation-key":"VaryWei2312.06109","DOI":"10.48550/arXiv.2312.06109","issued":{"date-parts":[["2023",12,10]]},"number":"arXiv:2312.06109","publisher":"arXiv","source":"arXiv.org","title":"Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models","title-short":"Vary","type":"article","URL":"http://arxiv.org/abs/2312.06109"},
  {"id":"VikParuchuriMarkerParuchuri2024","abstract":"Convert PDF to markdown quickly with high accuracy","accessed":{"date-parts":[["2024",7,17]]},"author":[{"family":"Paruchuri","given":"Vik"}],"citation-key":"VikParuchuriMarkerParuchuri2024","genre":"Python","issued":{"date-parts":[["2024",7,17]]},"license":"GPL-3.0","original-date":{"date-parts":[["2023",10,30]]},"source":"GitHub","title":"VikParuchuri/marker","type":"software","URL":"https://github.com/VikParuchuri/marker"},
  {"id":"BreezedeusPix2TextBreezedeus2024","abstract":"An Open-Source Python3 tool for recognizing layouts, tables, math formulas (LaTeX), and text in images, converting them into Markdown format. A free alternative to Mathpix, empowering seamless conversion of visual content into text-based representations. 80+ languages are supported.","accessed":{"date-parts":[["2024",7,17]]},"author":[{"family":"Breezedeus","given":""}],"citation-key":"BreezedeusPix2TextBreezedeus2024","genre":"Jupyter Notebook","issued":{"date-parts":[["2024",7,17]]},"license":"MIT","original-date":{"date-parts":[["2022",9,7]]},"source":"GitHub","title":"breezedeus/Pix2Text","type":"software","URL":"https://github.com/breezedeus/Pix2Text"},
  {"id":"SmallLanguageModelWei2401.12503a","abstract":"Playing Large Vision Language Models (LVLMs) in 2023 is trendy among the AI community. However, the relatively large number of parameters (more than 7B) of popular LVLMs makes it difficult to train and deploy on consumer GPUs, discouraging many researchers with limited resources. Imagine how cool it would be to experience all the features of current LVLMs on an old GTX1080ti (our only game card). Accordingly, we present Vary-toy in this report, a small-size Vary along with Qwen-1.8B as the base ``large'' language model. In Vary-toy, we introduce an improved vision vocabulary, allowing the model to not only possess all features of Vary but also gather more generality. Specifically, we replace negative samples of natural images with positive sample data driven by object detection in the procedure of generating vision vocabulary, more sufficiently utilizing the capacity of the vocabulary network and enabling it to efficiently encode visual information corresponding to natural objects. For experiments, Vary-toy can achieve 65.6% ANLS on DocVQA, 59.1% accuracy on ChartQA, 88.1% accuracy on RefCOCO, and 29% on MMVet. The code will be publicly available on the homepage.","accessed":{"date-parts":[["2024",7,18]]},"author":[{"family":"Wei","given":"Haoran"},{"family":"Kong","given":"Lingyu"},{"family":"Chen","given":"Jinyue"},{"family":"Zhao","given":"Liang"},{"family":"Ge","given":"Zheng"},{"family":"Yu","given":"En"},{"family":"Sun","given":"Jianjian"},{"family":"Han","given":"Chunrui"},{"family":"Zhang","given":"Xiangyu"}],"citation-key":"SmallLanguageModelWei2401.12503a","DOI":"10.48550/arXiv.2401.12503","issued":{"date-parts":[["2024",1,23]]},"number":"arXiv:2401.12503","publisher":"arXiv","source":"arXiv.org","title":"Small Language Model Meets with Reinforced Vision Vocabulary","type":"article","URL":"http://arxiv.org/abs/2401.12503"}
]
